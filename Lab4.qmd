---
title: "Lab4: Clustering (and PCA)"
author: "Mountain Dewds"
format: pdf
editor: visual
date: 2025-04-25
---

## **Due: 11:59PM Tuesday, May 5 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually create clusters from the `Attitude` data set we've worked with in class. You will create clusters of the data points (the respondents) AND also clusters of the attitudes.
2.  You will individually interpret the clusters of the data points and compare them to the established `Group` of respondents.
3.  As a team, you will decide the best way to cluster this data set. You will interpret these clusters.

### The Context

Four groups of individuals responded to a survey about Attitudes in Collaboration conducted by Prof. Vance and Laily Mualifah, Dr. Vance's colleague in Indonesia (where he spent the 2023-24 academic year as a Fulbright Scholar experimenting how to teach interdisciplinary collaboration to Indonesian statistics and data science students). The goal of the research project is to see what differences in attitudes are held by Indonesian students compared to Indonesian professionals compared to students in the U.S.

If you haven't taken the survey, you still can (for 3 bonus points) at https://forms.gle/Q7d5XVMeub5Y2DLN6. (Note: if you already took the survey you've already been given 5 bonus points. Don't take the survey a second time. If you do, your bonus score will change from 5 to 3.)

#### The Survey

The survey asks respondents to rate how often they have an attitude and then to what degree they think that attitude is detrimental to or beneficial for interdisciplinary collaboration. Then some demographic and experience questions are asked at the end of the survey.

The four groups who took the survey were:

-   Indonesian professionals (not undergraduate students) in 2024, labeled `IndoProf24`

-   Indonesian undergraduates in 2024 after taking a course on Statistical Collaboration, labeled `IndoPost24`

-   Students fromStatistical Collaboration at CU Boulder. Both ugrads and grad students, labeled `StatCollab`

-   Students from Statistical Learning at CU Boulder in Spring 2025, labeled `SLSp25`

#### The Data

The data can be downloaded from: <https://drive.google.com/file/d/179nQH4lO2oUR_9l8sySFKZI7RDPU5rMG/view?usp=sharing>

The questions were asked in groups of five. The first five questions were about how often the respondent had Attitudes 1-5, then how beneficial/detrimental Attitudes 1-5 would be for collaboration, then how often for Attitudes 6-10 and how beneficial/detrimental for Attitudes 6-10, and so on until Attitudes 21-25.

The column name "A1.freq" indicates Frequency of Attitude 1 (How often do you have this attitude?) on a scale from 1 (Never) to 5 (Always). The ratings numbers 1-5 correspond to the ratings "Never" "Rarely" "Sometimes" "Usually" and "Always" (and their Indonesian translations) for the Frequency questions.

The column name "A1.fav" indicates how detrimental/beneficial the respondent thinks that attitude is on a six-point scale: "Extremely Detrimental (1)" "Detrimental (2)" "Somewhat Detrimental (3)", "Somewhat Beneficial (4)", "Beneficial (5)", and "Extremely Beneficial (6)" (and their Indonesian translations).

The 25 attitudes are downloadable here: <https://docs.google.com/document/d/18Hjt4LPC2vkko9Bxm_7NqVgKYLEto8Mz/edit?usp=sharing&ouid=108624562691464780001&rtpof=true&sd=true>

The `Group` variable in the data set has values "StatCollab", "SLSp25", "IndoProf24", and "IndoPost24" corresponding to American students in Statistical Collaboration F24/Sp25, students in Statistical Learning Sp25, Indonesian professionals in 2024, and Indonesian undergraduates in 2024 after taking a course on Statistical Collaboration.

### Instructions for Lab4

#### What individuals need to do (20 pts)

## Josh Individual

1.  Implement K-means clustering on the data points. Use a sufficient number of starting values (20 is probably fine). Choose $K\in \{2,3,4,5,6\}$. Each individual should choose a different $K$.

    ```{r}
    df<-read.csv("Attitude.clean.csv")
    head(df)
    summary(df)
    library(tidyverse)
    dim(df)
    km.out <- kmeans(df[,2:50], 6, nstart = 20)

    par(mfrow = c(1, 2))
    ```

2.  Interpret your results. To what degree do your $K$ clusters correspond to the four groups?

    ```{r}

    #Outputting each cluster's group name
    table(km.out$cluster,df$Group)


    ```

    Using this output, we can see that the cluster's corresponded fairly well to actual group inclusion. No cluster had perfect classification but seemed to seperate statcollab and SlSP25 fairly well.

3.  For the same $K$, cluster the attitudes (cluster the transpose of the $X$ matrix). Interpret your results. Comment on the patterns you see within the clusters of attitudes.

    ```{r}
    #Clustering attitudes instead
    df<-read.csv("Attitude.clean.csv")
    #head(df)
    #summary(df)
    library(tidyverse)
    #dim(df)
    km.out <- kmeans(t(df[,2:50]), 6, nstart = 20)
    df_transpose<-t(df)



    #Outputting each cluster's group name

    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    rownames(df_transpose[km.out$cluster==1,])

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    rownames(df_transpose[km.out$cluster==2,])
    #Cluster 3
    cat("Groups in cluster 3", "\n","\n")
    rownames(df_transpose[km.out$cluster==3,])
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    rownames(df_transpose[km.out$cluster==4,])
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    rownames(df_transpose[km.out$cluster==5,])
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    rownames(df_transpose[km.out$cluster==6,])
    ```

    These clusters seem to show that the attitudes that are related are:

    A6, A11, A12, A13, A15, and A20 then we have

    A23 by itself

    A5, A2, A3, A4, A8, A15, A14, A17, A18

    A5, A10, A20, A24, A25

    A1, A9, A13, A16, A21, A25

    and A2, A3, A4, A1, A7, A9, A10, A14, A17, A18, A19, A6, A21, A24.

    Considering these clusters, it does not seem like the attitudes have any relation that can be found using this specific k means model except that A23 seems to be very different from the others.

4.  Use hierarchical clustering to cluster the data points. Each individual should use a different linkage (complete, single, average, or centroid). You must try at least two different distance measures (e.g., Euclidean, Manhattan, correlation). Comment on how many clusters you chose and why? Comment on which distance measure you used for your final clusters and why.

    Below I chose 4 clusters to try and fit how many groups I know there are for students respondents. I used euclidean distance as a measure because it seemed to link together similar attitudes together.I used manhattan distance because the responses only had integer values so a block count of distance seemed like it could be better.

    ```{r}
    #Using complete and euclidean
    hc.complete <- hclust(dist(df[,2:50],method="euclidean"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    cutree(hc.complete, 4)
    cat("4 Clusters when using euclidean distance","\n")
    table(cutree(hc.complete,4),df$Group)


    #Now using manhattan based distance
    hc.complete <- hclust(dist(df[,2:50],method="manhattan"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    #Output of clusters using manhattan based distance

    cat("4 Clusters when using manhattan distance","\n")
    table(cutree(hc.complete,4),df$Group)



    ```

    .

5.  Plot the "best" dendrogram. Interpret your results. To what degree do your $K$ clusters correspond to the four groups?

    The best dendogram seemed to be that resulting from euclidean distance. The clusters correspond to the groups not as well as k means. Below is the plot of this dendogram:

    ```{r}
    hc.complete <- hclust(dist(scale(df[,2:50])), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    ```

6.  Cluster the attitudes using hierarchical clustering. Use a different linkage (or use more than one and pick the best one). Justify your choice of distance measure.

    Below I used manhattan distance because the attitudes only had integer values.

    ```{r}
    #Used single linkage
    hc.complete <- hclust(dist(t(df[,2:50]),method="manhattan"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    cat("6 Clusters when using Manhattan distance","\n")
    #Used k=6 because it could mirror my k means work.
    cutree(hc.complete,k=6)
    ```

7.  Plot the "best" dendrogram. Interpret your results. Compare the patterns you see within the hierarchical clusters of attitudes with the patterns you saw using K-means.

    Below is the dendogram and the group assignments. These two clusters differ completely. Especially the point where we saw A23 as being distinct from the other groups is not present here.

    ```{r}
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==1,])

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==2,])
    #Cluster 3
    cat("Groups in cluster 3", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==3,])
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==4,])
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==5,])
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==6,])

    ```

#### What teams need to do

1.  Review and combine individual results. Which methods for clustering the data points seemed to correspond best with the pre-established groups? What $K$ gave you the best results? What linkage? What distance metric?

    # NOT SURE HOW TO COMBINE RESULTS. Maybe just print all clustering results for the groups and pick the "best" one?

2.  Compare the results from Principal Component Analysis (PCA) with the results of clustering the attitudes. Which is more interpretable? What is your interpretation?

    The clustering of attitudes seems to be much more interpretable because we can clearly see group assignments of which attitudes are linked together. PCA gives us a combination of attitudes, but it's difficult to identify grouping between all attitudes and where we should define our "cutoffs".

    Below are the results from PCA:

    ```{r}
    pr.out <- prcomp(t(df[,2:50]), scale=TRUE)
    names(pr.out)

    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)
    pve


    pr.out$rotation

    par(mfrow = c(1, 2))
    plot(pve, xlab = "Principal Component",ylab = "Proportion of Variance Explained", ylim = c(0, 1),type = "b")
    plot(cumsum(pve), xlab = "Principal Component", ylab ="Cumulative Proportion of Variance Explained",ylim = c(0, 1), type = "b")


    ```

    ```{r}
    #So we should just using the first two principal components

    pr.out <- prcomp(t(df[,2:50]), scale=TRUE)


    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)

    #using coefficents as measures of importance and relation
    cat("Which attitudes had high positive coefficents in PC1:","\n","\n")
    which((pr.out$x[,1])>7)
    cat("Which attitudes had high negative coefficents in PC1:","\n","\n")
    which(pr.out$x[,1] < -7)
    cat("\n","\n")
    cat("Which attitudes had high positive coefficents in PC2:","\n","\n")
    which((pr.out$x[,2])>3)
    cat("Which attitudes had high negative coefficents in PC2:","\n","\n")
    which(pr.out$x[,2] < -3)

    ```

    Using the PCA output for the first two principal components, which explained close to 40% of the variation in the data, we can see that some attitudes are scaled to be used higher in the principal components. Though the interpratability is difficult, we may attempt to interpret this as A6, A12, A16, A21, A25, A23 being related. Also, A1, A3, A4, A5, A9, A11, A15, A18, A19, A20 would be related. Next, A6, A7, A12, A13, A14, A16, A21 would be related. Finally, A2, A4, A5, A9, A10, and A18 would be related.

    Using PCA and clustering, we see that the fav aspect and freq aspect of each attitude are usally grouped together which makes sense. These aspects would probably be interpreted differently by respondents. Beyond that, it was difficult to find groupings that were similar between these methods. More observations may be needed to create reliable groupings.

    ## FINISH QUESTION 2 (What is our interpretation)

3.  Create your team's final, best clustering of the data points. How well do the clusters correspond to the four groups?

    Below is the best clustering of the data points for clustering the four groups:

    ```{r}

    ```

    The clustering of the groups seemed to be

    # Finish QUESTION 3

4.  Create your team's final, best clustering of the attitudes. What patterns do you see within the clusters of attitudes?

    ## Finish Question 4. Are there any patterns in the clusters of attitudes?

5.  Write a paragraph about individual contributions. Each individual must comment on their contributions to the lab. Only individuals who contribute to the team section will get points for the team section. An example contribution could be: "I synthesized all of the individual results to find that such and such gave the best and most interpretable results. Then I recommended teammate X use this linkage and that distance measure for our team's cluster." Or, "I did the comparison of PCA with our clustering of the attitudes." Or, "I created our team's best clustering of the data points based on teammate Y's synthesis of our individual results." Or, "I took everyone's code and put it together into the qmd file and knitted it." Or, "I made the plot of our team's best clusters."

## When done we can write contributions here:

Josh: I created a PCA and compared it's interpretability to the clustering methods. I also contributed to the interpretation section of question 2.

#### Potentially useful code below

```{r include=F}
suppressPackageStartupMessages(library(tidyverse))
dat <- read_csv("Attitude.clean.csv")
dim(dat)

X <- dat[,-1] #Excluding the groups
dim(X)

# Creating distance measures
cor_spearman <- cor(t(X), method = "spearman") # Transpose so the rows are the attitude variables
dist_spearman <- as.dist(1 - cor_spearman)

cor_pearson <- cor(t(X), method = "pearson") 
dist_pearson <- as.dist(1 - cor_pearson)

# Using Manhattan or Euclidean distance
dist_manhattan <- dist(X, method = "manhattan") #Manhattan distance counts 
# the "blocks" between data points as if you were walking along the streets of 
# Manhattan (i.e., you can't cut diagonally through a city block)
dist_euclidean <- dist(X, method = "euclidean") #Euclidean distance is the 
# diagonal "shortest path" between points A and B.

# Using hclust
hc.complete <- hclust(dist_spearman, method = "complete")
cutree(hc.complete, k=4)

# Kmeans
km.out <- kmeans(X, k=4, nstart = 20)

# Doing PCA
pr.out <- prcomp(X,scale=F) #The attitudes are basically already on the same scale, so scaling the attitudes would confuse things, though A1.freq is on a 5-point scale and A1.fav is on a 6-point scale

# Find means of the four groups
SL.means <- colMeans(dat[dat[,1]=="SLSp25",2:51])
StatCollab.means <- colMeans(dat[dat[,1]=="StatCollab",2:51])
IndoProf.means <- colMeans(dat[dat[,1]=="IndoProf24",2:51])
IndoPost.means <- colMeans(dat[dat[,1]=="IndoPost24",2:51])

# Then transform the group centroids into the Principal Component space
C.matrix <- rbind(StatCollab.means,SL.means,IndoProf.means,IndoPost.means)

#Transform C.matrix into scores using the rotation matrix of the first two principal components
W2 <- pr.out$rotation[,1:2]
Group.scores <- C.matrix %*% W2
Group.scores <- mutate(as.data.frame(Group.scores),groups=rownames(Group.scores))
ggplot(data=Group.scores, aes(x=PC1,y=PC2,col=groups)) +
  geom_point()

# Comparing found clusters with groups
# What percentage of each cluster is each group?
table(km.out$cluster,dat$Group) #can transform this into percentages if necessary

# We can do the same with clusters found from hiearchical clustering
table(cutree(hc.complete,k=4),dat$Group) #These clusters aren't very good.



```
