---
title: "Lab4: Clustering (and PCA)"
author: "Mountain Dewds"
format: pdf
editor: visual
date: 2025-04-25
---

## **Due: 11:59PM Tuesday, May 5 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually create clusters from the `Attitude` data set we've worked with in class. You will create clusters of the data points (the respondents) AND also clusters of the attitudes.
2.  You will individually interpret the clusters of the data points and compare them to the established `Group` of respondents.
3.  As a team, you will decide the best way to cluster this data set. You will interpret these clusters.

### The Context

Four groups of individuals responded to a survey about Attitudes in Collaboration conducted by Prof. Vance and Laily Mualifah, Dr. Vance's colleague in Indonesia (where he spent the 2023-24 academic year as a Fulbright Scholar experimenting how to teach interdisciplinary collaboration to Indonesian statistics and data science students). The goal of the research project is to see what differences in attitudes are held by Indonesian students compared to Indonesian professionals compared to students in the U.S.

If you haven't taken the survey, you still can (for 3 bonus points) at https://forms.gle/Q7d5XVMeub5Y2DLN6. (Note: if you already took the survey you've already been given 5 bonus points. Don't take the survey a second time. If you do, your bonus score will change from 5 to 3.)

#### The Survey

The survey asks respondents to rate how often they have an attitude and then to what degree they think that attitude is detrimental to or beneficial for interdisciplinary collaboration. Then some demographic and experience questions are asked at the end of the survey.

The four groups who took the survey were:

-   Indonesian professionals (not undergraduate students) in 2024, labeled `IndoProf24`

-   Indonesian undergraduates in 2024 after taking a course on Statistical Collaboration, labeled `IndoPost24`

-   Students fromStatistical Collaboration at CU Boulder. Both ugrads and grad students, labeled `StatCollab`

-   Students from Statistical Learning at CU Boulder in Spring 2025, labeled `SLSp25`

#### The Data

The data can be downloaded from: <https://drive.google.com/file/d/179nQH4lO2oUR_9l8sySFKZI7RDPU5rMG/view?usp=sharing>

The questions were asked in groups of five. The first five questions were about how often the respondent had Attitudes 1-5, then how beneficial/detrimental Attitudes 1-5 would be for collaboration, then how often for Attitudes 6-10 and how beneficial/detrimental for Attitudes 6-10, and so on until Attitudes 21-25.

The column name "A1.freq" indicates Frequency of Attitude 1 (How often do you have this attitude?) on a scale from 1 (Never) to 5 (Always). The ratings numbers 1-5 correspond to the ratings "Never" "Rarely" "Sometimes" "Usually" and "Always" (and their Indonesian translations) for the Frequency questions.

The column name "A1.fav" indicates how detrimental/beneficial the respondent thinks that attitude is on a six-point scale: "Extremely Detrimental (1)" "Detrimental (2)" "Somewhat Detrimental (3)", "Somewhat Beneficial (4)", "Beneficial (5)", and "Extremely Beneficial (6)" (and their Indonesian translations).

The 25 attitudes are downloadable here: <https://docs.google.com/document/d/18Hjt4LPC2vkko9Bxm_7NqVgKYLEto8Mz/edit?usp=sharing&ouid=108624562691464780001&rtpof=true&sd=true>

The `Group` variable in the data set has values "StatCollab", "SLSp25", "IndoProf24", and "IndoPost24" corresponding to American students in Statistical Collaboration F24/Sp25, students in Statistical Learning Sp25, Indonesian professionals in 2024, and Indonesian undergraduates in 2024 after taking a course on Statistical Collaboration.

### Instructions for Lab4

#### What individuals need to do (20 pts)

## Josh Individual

1.  Implement K-means clustering on the data points. Use a sufficient number of starting values (20 is probably fine). Choose $K\in \{2,3,4,5,6\}$. Each individual should choose a different $K$.

    ```{r}
    df<-read.csv("Attitude.clean.csv")
    head(df)
    summary(df)
    library(tidyverse)
    dim(df)
    km.out <- kmeans(df[,2:50], 6, nstart = 20)

    par(mfrow = c(1, 2))
    ```

2.  Interpret your results. To what degree do your $K$ clusters correspond to the four groups?

    ```{r}

    #Outputting each cluster's group name

    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    df[km.out$cluster==1,1]

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    df[km.out$cluster==2,1]

    #Clutser 3
    cat("Groups in cluster 3", "\n","\n")
    df[km.out$cluster==3,1]
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    df[km.out$cluster==4,1]
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    df[km.out$cluster==5,1]
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    df[km.out$cluster==6,1]

    ```

    Using this output, we can see that the cluster's corresponded fairly well to actual group inclusion. Cluster 5 was the only with perfect classification, but the other clusters only had relatively few classifications each.

3.  For the same $K$, cluster the attitudes (cluster the transpose of the $X$ matrix). Interpret your results. Comment on the patterns you see within the clusters of attitudes.

    ```{r}
    #Clustering attitudes instead
    df<-read.csv("Attitude.clean.csv")
    #head(df)
    #summary(df)
    library(tidyverse)
    #dim(df)
    km.out <- kmeans(t(df[,2:50]), 6, nstart = 20)
    df_transpose<-t(df)

    #Outputting each cluster's group name

    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    rownames(df_transpose[km.out$cluster==1,])

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    rownames(df_transpose[km.out$cluster==2,])
    #Cluster 3
    cat("Groups in cluster 3", "\n","\n")
    rownames(df_transpose[km.out$cluster==3,])
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    rownames(df_transpose[km.out$cluster==4,])
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    rownames(df_transpose[km.out$cluster==5,])
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    rownames(df_transpose[km.out$cluster==6,])
    ```

    These clusters seem to show that the attitudes that are related are:

    A6, A11, A12, A13, A15, and A20 then we have

    A23 by itself

    A5, A2, A3, A4, A8, A15, A14, A17, A18

    A5, A10, A20, A24, A25

    A1, A9, A13, A16, A21, A25

    and A2, A3, A4, A1, A7, A9, A10, A14, A17, A18, A19, A6, A21, A24.

    Considering these clusters, it does not seem like the attitudes have any relation that can be found using this specific k means model except that A23 seems to be very different from the others.

4.  Use hierarchical clustering to cluster the data points. Each individual should use a different linkage (complete, single, average, or centroid). You must try at least two different distance measures (e.g., Euclidean, Manhattan, correlation). Comment on how many clusters you chose and why? Comment on which distance measure you used for your final clusters and why.

    Below I chose 4 clusters to try and fit how many groups I know there are for students respondents. I used euclidean distance as a measure because it seemed to link together similar attitudes together.I used manhattan distance because the responses only had integer values so a block count of distance seemed like it could be better.

    ```{r}
    #Using complete and euclidean
    hc.complete <- hclust(dist(df[,2:50],method="euclidean"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    cat("4 Clusters when using euclidean distance","\n")



    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    df[cutree(hc.complete, 4)==1,1]
    #cluster 1
    cat("Groups in cluster 2", "\n","\n")
    df[cutree(hc.complete, 4)==2,1]
    #cluster 1
    cat("Groups in cluster 3", "\n","\n")
    df[cutree(hc.complete, 4)==3,1]
    #cluster 1
    cat("Groups in cluster 4", "\n","\n")
    df[cutree(hc.complete, 4)==4,1]

    #Now using manhattan based distance
    hc.complete <- hclust(dist(df[,2:50],method="manhattan"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    #Output of clusters using manhattan based distance

    cat("4 Clusters when using manhattan distance","\n")

    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    df[cutree(hc.complete, 4)==1,1]
    #cluster 1
    cat("Groups in cluster 2", "\n","\n")
    df[cutree(hc.complete, 4)==2,1]
    #cluster 1
    cat("Groups in cluster 3", "\n","\n")
    df[cutree(hc.complete, 4)==3,1]
    #cluster 1
    cat("Groups in cluster 4", "\n","\n")
    df[cutree(hc.complete, 4)==4,1]

    ```

    .

5.  Plot the "best" dendrogram. Interpret your results. To what degree do your $K$ clusters correspond to the four groups?

    The best dedogram seemed to be that resulting from euclidean distance. The clusters correspond to the groups not as well as k means. Below is the plot of this dendogram:

    ```{r}
    hc.complete <- hclust(dist(scale(df[,2:50])), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    ```

6.  Cluster the attitudes using hierarchical clustering. Use a different linkage (or use more than one and pick the best one). Justify your choice of distance measure.

    Below I used manhattan distance because the attitudes only had integer values.

    ```{r}
    #Used single linkage
    hc.complete <- hclust(dist(t(df[,2:50]),method="manhattan"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    cat("6 Clusters when using Manhattan distance","\n")
    #Used k=6 because it could mirror my k means work.
    cutree(hc.complete,k=6)
    ```

7.  Plot the "best" dendrogram. Interpret your results. Compare the patterns you see within the hierarchical clusters of attitudes with the patterns you saw using K-means.

    Below is the dendogram and the group assignments. These two clusters differ completely. Especially the point where we saw A23 as being distinct from the other groups is not present here.

    ```{r}
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==1,])

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==2,])
    #Cluster 3
    cat("Groups in cluster 3", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==3,])
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==4,])
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==5,])
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==6,])

    ```

#### What teams need to do

1.  Review and combine individual results. Which methods for clustering the data points seemed to correspond best with the pre-established groups? What $K$ gave you the best results? What linkage? What distance metric?

    # NOT SURE HOW TO COMBINE RESULTS. Maybe just print all clustering results for the groups and pick the "best" one?

2.  Compare the results from Principal Component Analysis (PCA) with the results of clustering the attitudes. Which is more interpretable? What is your interpretation?

    The clustering of attitudes seems to be much more interpretable because we can clearly see group assignments of which attitudes are linked together. PCA gives us a combination of attitudes, but it's difficult to identify grouping between all attitudes and where we should define our "cutoffs".

    Below are the results from PCA:

    ```{r}
    pr.out <- prcomp(df[,2:50], scale=TRUE)
    names(pr.out)

    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)
    pve


    pr.out$rotation

    par(mfrow = c(1, 2))
    plot(pve, xlab = "Principal Component",ylab = "Proportion of Variance Explained", ylim = c(0, 1),type = "b")
    plot(cumsum(pve), xlab = "Principal Component", ylab ="Cumulative Proportion of Variance Explained",ylim = c(0, 1), type = "b")

    ##Considering only Indonesians
    pr.out <- prcomp(df[28:93,2:50], scale=TRUE)
    names(pr.out)

    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)
    pve

    pr.out$rotation

    par(mfrow = c(1, 2))
    plot(pve, xlab = "Principal Component",ylab = "Proportion of Variance Explained", ylim = c(0, 1),type = "b")
    plot(cumsum(pve), xlab = "Principal Component", ylab ="Cumulative Proportion of Variance Explained",ylim = c(0, 1), type = "b")

    ##Considering only CU Boulder
    pr.out <- prcomp(df[1:27,2:50], scale=TRUE)
    names(pr.out)

    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)
    pve

    pr.out$rotation

    par(mfrow = c(1, 2))
    plot(pve, xlab = "Principal Component",ylab = "Proportion of Variance Explained", ylim = c(0, 1),type = "b")
    plot(cumsum(pve), xlab = "Principal Component", ylab ="Cumulative Proportion of Variance Explained",ylim = c(0, 1), type = "b")

    pr.out$x

    ```

    ## FINISH QUESTION 2 (What is our interpretation)

3.  Create your team's final, best clustering of the data points. How well do the clusters correspond to the four groups?

    Below is the best clustering of the data points for clustering the four groups:

    ```{r}

    ```

    The clustering of the groups seemed to be

    # Finish QUESTION 3

4.  Create your team's final, best clustering of the attitudes. What patterns do you see within the clusters of attitudes?

    ## Finish Question 4. Are there any patterns in the clusters of attitudes?

5.  Write a paragraph about individual contributions. Each individual must comment on their contributions to the lab. Only individuals who contribute to the team section will get points for the team section. An example contribution could be: "I synthesized all of the individual results to find that such and such gave the best and most interpretable results. Then I recommended teammate X use this linkage and that distance measure for our team's cluster." Or, "I did the comparison of PCA with our clustering of the attitudes." Or, "I created our team's best clustering of the data points based on teammate Y's synthesis of our individual results." Or, "I took everyone's code and put it together into the qmd file and knitted it." Or, "I made the plot of our team's best clusters."

## When done we can write contributions here:

Josh: I created a PCA

#### Some intended outcomes from this assignment:

1.  You will individually get practice using K-means and hierarchical clustering
2.  You will practice using different distance measures and linkages for hierarchical clustering
3.  You will gain experience interpreting the outputs of clustering algorithms
4.  You will gain experience collaborating with your teammates on an applied problem

#### Potentially useful code below

```{r include=F}
suppressPackageStartupMessages(library(tidyverse))
dat <- read_csv("Attitude.clean.csv")
dim(dat)

X <- dat[,-1] #Excluding the groups
dim(X)

# Creating distance measures
cor_spearman <- cor(t(X), method = "spearman") # Transpose so the rows are the attitude variables
dist_spearman <- as.dist(1 - cor_spearman)

cor_pearson <- cor(t(X), method = "pearson") 
dist_pearson <- as.dist(1 - cor_pearson)

# Using Manhattan or Euclidean distance
dist_manhattan <- dist(X, method = "manhattan") #Manhattan distance counts 
# the "blocks" between data points as if you were walking along the streets of 
# Manhattan (i.e., you can't cut diagonally through a city block)
dist_euclidean <- dist(X, method = "euclidean") #Euclidean distance is the 
# diagonal "shortest path" between points A and B.

# Using hclust
hc.complete <- hclust(dist_spearman, method = "complete")
# plot(hc.complete, main = "Complete Linkage", sub="Spearman Corr Distance", xlab = "", cex = .9)

# Comparing found clusters with groups



```
