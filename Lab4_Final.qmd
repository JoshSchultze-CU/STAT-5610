---
title: "Lab4: Clustering (and PCA)"
author: "Mountain Dewds"
format: pdf
editor: visual
date: 2025-04-25
---

## **Due: 11:59PM Tuesday, May 5 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually create clusters from the `Attitude` data set we've worked with in class. You will create clusters of the data points (the respondents) AND also clusters of the attitudes.
2.  You will individually interpret the clusters of the data points and compare them to the established `Group` of respondents.
3.  As a team, you will decide the best way to cluster this data set. You will interpret these clusters.

### The Context

Four groups of individuals responded to a survey about Attitudes in Collaboration conducted by Prof. Vance and Laily Mualifah, Dr. Vance's colleague in Indonesia (where he spent the 2023-24 academic year as a Fulbright Scholar experimenting how to teach interdisciplinary collaboration to Indonesian statistics and data science students). The goal of the research project is to see what differences in attitudes are held by Indonesian students compared to Indonesian professionals compared to students in the U.S.

If you haven't taken the survey, you still can (for 3 bonus points) at https://forms.gle/Q7d5XVMeub5Y2DLN6. (Note: if you already took the survey you've already been given 5 bonus points. Don't take the survey a second time. If you do, your bonus score will change from 5 to 3.)

#### The Survey

The survey asks respondents to rate how often they have an attitude and then to what degree they think that attitude is detrimental to or beneficial for interdisciplinary collaboration. Then some demographic and experience questions are asked at the end of the survey.

The four groups who took the survey were:

-   Indonesian professionals (not undergraduate students) in 2024, labeled `IndoProf24`

-   Indonesian undergraduates in 2024 after taking a course on Statistical Collaboration, labeled `IndoPost24`

-   Students fromStatistical Collaboration at CU Boulder. Both ugrads and grad students, labeled `StatCollab`

-   Students from Statistical Learning at CU Boulder in Spring 2025, labeled `SLSp25`

#### The Data

The data can be downloaded from: <https://drive.google.com/file/d/179nQH4lO2oUR_9l8sySFKZI7RDPU5rMG/view?usp=sharing>

The questions were asked in groups of five. The first five questions were about how often the respondent had Attitudes 1-5, then how beneficial/detrimental Attitudes 1-5 would be for collaboration, then how often for Attitudes 6-10 and how beneficial/detrimental for Attitudes 6-10, and so on until Attitudes 21-25.

The column name "A1.freq" indicates Frequency of Attitude 1 (How often do you have this attitude?) on a scale from 1 (Never) to 5 (Always). The ratings numbers 1-5 correspond to the ratings "Never" "Rarely" "Sometimes" "Usually" and "Always" (and their Indonesian translations) for the Frequency questions.

The column name "A1.fav" indicates how detrimental/beneficial the respondent thinks that attitude is on a six-point scale: "Extremely Detrimental (1)" "Detrimental (2)" "Somewhat Detrimental (3)", "Somewhat Beneficial (4)", "Beneficial (5)", and "Extremely Beneficial (6)" (and their Indonesian translations).

The 25 attitudes are downloadable here: <https://docs.google.com/document/d/18Hjt4LPC2vkko9Bxm_7NqVgKYLEto8Mz/edit?usp=sharing&ouid=108624562691464780001&rtpof=true&sd=true>

The `Group` variable in the data set has values "StatCollab", "SLSp25", "IndoProf24", and "IndoPost24" corresponding to American students in Statistical Collaboration F24/Sp25, students in Statistical Learning Sp25, Indonesian professionals in 2024, and Indonesian undergraduates in 2024 after taking a course on Statistical Collaboration.

### Instructions for Lab4

#### What individuals need to do (20 pts)

## Josh Individual

1.  Implement K-means clustering on the data points. Use a sufficient number of starting values (20 is probably fine). Choose $K\in \{2,3,4,5,6\}$. Each individual should choose a different $K$.

    ```{r}
    df<-read.csv("Attitude.clean.csv")
    head(df)
    summary(df)
    library(tidyverse)
    dim(df)
    km.out <- kmeans(df[,2:50], 6, nstart = 20)

    par(mfrow = c(1, 2))
    ```

2.  Interpret your results. To what degree do your $K$ clusters correspond to the four groups?

    ```{r}

    #Outputting each cluster's group name
    table(km.out$cluster,df$Group)


    ```

    Using this output, we can see that the cluster's corresponded fairly well to actual group inclusion. No cluster had perfect classification but seemed to seperate statcollab and SlSP25 fairly well.

3.  For the same $K$, cluster the attitudes (cluster the transpose of the $X$ matrix). Interpret your results. Comment on the patterns you see within the clusters of attitudes.

    ```{r}
    #Clustering attitudes instead
    df<-read.csv("Attitude.clean.csv")
    #head(df)
    #summary(df)
    library(tidyverse)
    #dim(df)
    km.out <- kmeans(t(df[,2:50]), 6, nstart = 20)
    df_transpose<-t(df)



    #Outputting each cluster's group name

    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    rownames(df_transpose[km.out$cluster==1,])

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    rownames(df_transpose[km.out$cluster==2,])
    #Cluster 3
    cat("Groups in cluster 3", "\n","\n")
    rownames(df_transpose[km.out$cluster==3,])
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    rownames(df_transpose[km.out$cluster==4,])
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    rownames(df_transpose[km.out$cluster==5,])
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    rownames(df_transpose[km.out$cluster==6,])
    ```

    These clusters seem to show that the attitudes that are related are:

    A6, A11, A12, A13, A15, and A20 then we have

    A23 by itself

    A5, A2, A3, A4, A8, A15, A14, A17, A18

    A5, A10, A20, A24, A25

    A1, A9, A13, A16, A21, A25

    and A2, A3, A4, A1, A7, A9, A10, A14, A17, A18, A19, A6, A21, A24.

    Considering these clusters, it does not seem like the attitudes have any relation that can be found using this specific k means model except that A23 seems to be very different from the others.

4.  Use hierarchical clustering to cluster the data points. Each individual should use a different linkage (complete, single, average, or centroid). You must try at least two different distance measures (e.g., Euclidean, Manhattan, correlation). Comment on how many clusters you chose and why? Comment on which distance measure you used for your final clusters and why.

    Below I chose 4 clusters to try and fit how many groups I know there are for students respondents. I used euclidean distance as a measure because it seemed to link together similar attitudes together .I used manhattan distance because the responses only had integer values so a block count of distance seemed like it could be better.

    ```{r}
    #Using complete and euclidean
    hc.complete <- hclust(dist(df[,2:50],method="euclidean"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    cutree(hc.complete, 4)
    cat("4 Clusters when using euclidean distance","\n")
    table(cutree(hc.complete,4),df$Group)


    #Now using manhattan based distance
    hc.complete <- hclust(dist(df[,2:50],method="manhattan"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    #Output of clusters using manhattan based distance

    cat("4 Clusters when using manhattan distance","\n")
    table(cutree(hc.complete,4),df$Group)



    ```

    .

5.  Plot the "best" dendrogram. Interpret your results. To what degree do your $K$ clusters correspond to the four groups?

    The best dendogram seemed to be that resulting from euclidean distance. The clusters correspond to the groups not as well as k means and the cluster output can be seen in the cell above. Below is the plot of this dendogram:

    ```{r}
    hc.complete <- hclust(dist(scale(df[,2:50])), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    ```

6.  Cluster the attitudes using hierarchical clustering. Use a different linkage (or use more than one and pick the best one). Justify your choice of distance measure.

    Below I used manhattan distance because the attitudes only had integer values.

    ```{r}
    #Used single linkage
    hc.complete <- hclust(dist(t(df[,2:50]),method="manhattan"), method = "complete")
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)

    cat("6 Clusters when using Manhattan distance","\n")
    #Used k=6 because it could mirror my k means work.
    cutree(hc.complete,k=6)
    ```

7.  Plot the "best" dendrogram. Interpret your results. Compare the patterns you see within the hierarchical clusters of attitudes with the patterns you saw using K-means.

    Below is the dendogram and the group assignments. These two clusters differ completely. Especially the point where we saw A23 as being distinct from the other groups is not present here. It seems that these methods result in totally different clusters.

    ```{r}
    par(mfrow = c(1, 3))
    plot(hc.complete, main = "Complete Linkage",xlab = "", sub = "", cex = .9)
    #cluster 1
    cat("Groups in cluster 1", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==1,])

    #Cluster 2
    cat("Groups in cluster 2", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==2,])
    #Cluster 3
    cat("Groups in cluster 3", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==3,])
    #Cluster 4
    cat("Groups in cluster 4", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==4,])
    #Cluster 5
    cat("Groups in cluster 5", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==5,])
    #Cluster 6
    cat("Groups in cluster 6", "\n","\n")
    rownames(df_transpose[cutree(hc.complete,k=6)==6,])

    ```

# Will Individual

## 1.

```{r}
    df<-read.csv("Attitude.clean.csv")
    head(df)
    summary(df)
    library(tidyverse)
    dim(df)
    km.out <- kmeans(df[,2:50], 3, nstart = 20)
    par(mfrow = c(1, 2))
```

## 2.

The clusters don’t line up well with the original groups. Cluster 1 is mostly Group 2, Cluster 2 is a mix of Groups 1 and 2, and Cluster 3 only has a few points from Group 1. Group 3 barely shows up at all. This suggests the k-means clustering didn’t capture the original group structure, probably because the features don’t separate the groups clearly.

```{r}
    table(km.out$cluster,df$Group)
```

## 3.

When clustering the attitudes themselves instead of the people, we can see that certain survey items naturally grouped together. Cluster 1 grouped frequency questions and a few favorability ones, probably because they’re all about how often something is done or how common it is. Cluster 2 focused more on favorability, including questions like A2, A3, A4, and A15, so this group might reflect positive perception more directly. Cluster 3 is a mixed bag, but it includes a lot of the remaining frequency and favorability items that didn’t clearly fit the other two themes. It seems like the clusters separated questions based on type favorability vs. frequency.

```{r}
df<-read.csv("Attitude.clean.csv")
library(tidyverse)
km.out <- kmeans(t(df[,2:50]), 3, nstart = 20)
df_transpose<-t(df)
cat("Groups in cluster 1", "\n","\n")
rownames(df_transpose[km.out$cluster==1,])
cat("Groups in cluster 2", "\n","\n")
rownames(df_transpose[km.out$cluster==2,])
cat("Groups in cluster 3", "\n","\n")
rownames(df_transpose[km.out$cluster==3,])

```

## 4.

I used 4 clusters because there are 4 groups that we are trying to classify. I ran hierarchical clustering using both complete linkage with Euclidean distance and average linkage with Manhattan distance. The complete method gave me four clusters with sizes 72, 10, 4, and 7. The average method also gave four clusters but one of them only had a single point and the others were 84, 3, and 5. I went with the complete linkage because the groups were more balanced and made more sense overall. The average method felt too sensitive and seemed to overreact to outliers.

```{r}
df_scaled <- scale(df[, 2:50]) 
dist_euc <- dist(df_scaled, method = "euclidean")
dist_man <- dist(df_scaled, method = "manhattan")

# Hierarchical clustering with two linkage methods
hc_complete <- hclust(dist_euc, method = "complete")
hc_average <- hclust(dist_man, method = "average")

# Plot dendrograms
plot(hc_complete, main = "Complete Linkage - Euclidean")
plot(hc_average, main = "Average Linkage - Manhattan")

# Cut dendrograms to choose k = 3 clusters
clusters_complete <- cutree(hc_complete, k = 4)
clusters_average <- cutree(hc_average, k = 4)

# Compare clusters
table(clusters_complete)
table(clusters_average)
```

## 5.

The dendrogram using complete linkage separated the data into four pretty distinct clusters. Cluster 1 had the majority of people from each group, but especially IndoPost24 and IndoProf24. That suggests these two groups are more similar to each other than the rest. Cluster 2 had just a few from the SCS and SLS groups, and clusters 3 and 4 had a couple IndoPost24 and IndoProf24 individuals. So overall, the hierarchical clustering kind of captured the general structure of the four original groups, but it mixed them together more than I expected.

```{r}
dist_euc <- dist(df_scaled, method = "euclidean")
hc_complete <- hclust(dist_euc, method = "complete")
plot(hc_complete, main = "Dendrogram - Complete Linkage (Individuals)", xlab = "", sub = "")
rect.hclust(hc_complete, k = 4, border = "red") 
cut_clusters <- cutree(hc_complete, k = 4)
table(cut_clusters, df$Group) 

```

## 6.

I clustered the attitudes using hierarchical clustering on the transposed data. I tested both Euclidean and correlation distance and ended up going with correlation since it captured more meaningful relationships between variables. I used average linkage because it grouped similar attitudes together without chaining or splitting too aggressively. The resulting clusters grouped attitudes that tended to move together across individuals, which gave a better sense of underlying patterns.

```{r}
df_att <- df[, 2:50]

# Standardize data
df_scaled <- scale(df_att)
df_t <- t(df_att)  # Transpose the data
df_t_scaled <- scale(df_t)
dist_corr <- as.dist(1 - cor(t(df_t_scaled)))
hc_att_avg <- hclust(dist_corr, method = "average")

```

## 7.

The dendrogram for the attitudes using average linkage and correlation distance gave groupings that matched up pretty well with what I saw from the k-means clusters. A lot of the same attitudes that were grouped together before showed up in the same branches here too. This confirms that the patterns we picked up with k-means weren’t random and are consistent across different clustering approaches.

```{r}
plot(hc_att_avg, main = "Dendrogram - Average Linkage (Attitudes)", xlab = "", sub = "")
rect.hclust(hc_att_avg, k = 4, border = "blue")  # Adjust k as needed
```

# Rishi Individual

1\.

```{r}
library(tidyverse)

data <- read.csv("Attitude.clean.csv")
X <- data %>% select(-Group)
X_scaled <- scale(X)

set.seed(42)
kmeans_result <- kmeans(X_scaled, centers = 5, nstart = 20)
data$cluster <- kmeans_result$cluster

table(data$Group, data$cluster)

```

2.  The K-means clustering with K = 5 showed partial alignment with the actual groups. For example, Cluster 4 mostly captured IndoProf24 respondents, while Cluster 1 had a concentration of IndoPost24. Other clusters were more mixed, suggesting some shared attitude patterns across different respondent groups.

3.  

    ```{r}
    X_t <- t(X_scaled)
    set.seed(42)
    kmeans_attitudes <- kmeans(X_t, centers = 5, nstart = 20)

    attitude_clusters <- data.frame(
      Attitude = colnames(X),
      Cluster = kmeans_attitudes$cluster
    )

    attitude_clusters <- attitude_clusters %>% arrange(Cluster)
    print(attitude_clusters)

    ```

The clustering of attitudes grouped together both frequency and favorability scores for the same or similar attitude numbers. This shows that the respondents who experience certain attitudes also tend to view them similarly in terms of how beneficial or sometimes detrimental they are.

4.  

    ```{r}
    # Euclidean distance + single linkage
    dist_euc <- dist(X_scaled, method = "euclidean")
    hc_single_euc <- hclust(dist_euc, method = "single")
    plot(hc_single_euc, labels = data$Group, main = "Dendrogram: Single Linkage (Euclidean)")

    # Manhattan distance + single linkage
    dist_manh <- dist(X_scaled, method = "manhattan")
    hc_single_manh <- hclust(dist_manh, method = "single")
    plot(hc_single_manh, labels = data$Group, main = "Dendrogram: Single Linkage (Manhattan)")

    # Cut the tree to create 5 clusters (same K)
    data$hclust_cluster <- cutree(hc_single_euc, k = 5)
    table(data$Group, data$hclust_cluster)

    ```

I picked 5 clusters to match what I used before with K-means and because there are 4 main groups in the data, plus maybe one extra mixed group. I chose Euclidean distance in the end because the clusters looked more clear and easier to tell apart in the chart. The Manhattan one was messier and harder to understand.

5.  

    ```{r}
    plot(hc_single_euc, labels = data$Group, main = "Best Dendrogram: Single Linkage (Euclidean)", xlab = "", sub = "", cex = 0.7)

    ```

From the dendrogram, we can see that some groups like IndoProf24 are grouped together in certain branches, showing that the clustering picked up on some real patterns. Many other branches mix different groups, meaning the clusters don’t perfectly match the original four groups. The K = 5 clusters only partly reflect the actual group labels.

6.  

    ```{r}
    X_t <- t(X_scaled)
    cor_dist <- as.dist(1 - cor(X_t))
    hc_attitudes <- hclust(cor_dist, method = "average")
    plot(hc_attitudes, main = "Dendrogram of Attitudes (Correlation + Average)", xlab = "", sub = "", cex = 0.7)

    ```

I used average linkage because it gave smoother and more balanced clusters compared to complete or single linkage. I chose correlation distance because it focuses on how similar the patterns of the attitudes are across respondents.

7.  

    ```{r}
    plot(hc_attitudes, main = "Best Dendrogram: Attitudes (Correlation + Average)", xlab = "", sub = "", cex = 0.7)

    ```

The dendrogram shows clear groupings of attitudes, where similar .freq and .fav variables are often placed in the same cluster. This supports what we saw with K-means attitudes that people experience often are usually seen in a similar way.

# Chase Individual

1.  

```{r}
kmean <- kmeans(df[,2:50], 4, nstart = 20)

par(mfrow = c(1, 2)) 
table(kmean$cluster,df$Group)
```

2.  

```{r}
kmean <- kmeans(df[,2:50], 4, nstart = 20)

par(mfrow = c(1, 2)) 
table(kmean$cluster,df$Group)
```

3.  The attitudes are grouped into 4 clusters based on their frequencies and preferences. Cluster 1 contains attitudes like A23.fav and A24.fav, focused on preferences. Cluster 2 includes attitudes such as A1.fav, A3.fav, and A4.fav, which represent a different set of preferences. Cluster 3 groups attitudes like A1.freq, A3.freq, and A4.freq, which are more about frequency. Cluster 4 includes a mix of both frequency and preference attitudes, such as A9.fav, A10.fav, and A15.fav. Overall, similar types of attitudes (preference vs. frequency) are clustered together.

```{r}
library(dplyr)
attitude_data <- df[, !(names(df) %in% c("Name", "Group"))]
attitudes_t <- t(attitude_data)
# K=4 cluster
set.seed(44)
attitude_kmeans <- kmeans(attitudes_t, centers = 4, nstart = 20)
# Create df of attitude clusters
attitude_clusters_df <- data.frame(Attitude = rownames(attitudes_t), Cluster = attitude_kmeans$cluster)
# Rename columns to be clear
colnames(attitude_clusters_df) <- c("Attitude", "Cluster")
# Sort by cluster
attitude_clusters_df <- attitude_clusters_df %>% arrange(Cluster)
print(attitude_clusters_df)
```

4.  I picked 4 clusters because the dendrogram showed clear breaks between the groups. Euclidean distance worked best for the final clusters because it gave more understandable and clear groupings compared to Manhattan distance.

```{r}
data_matrix <- df %>% select(-Group)

# Compute distance matrices
dist_euclidean <- dist(data_matrix, method = "euclidean")
dist_manhattan <- dist(data_matrix, method = "manhattan")

# Hierarchical clustering with average linkage
hc_euclidean <- hclust(dist_euclidean, method = "average")
hc_manhattan <- hclust(dist_manhattan, method = "average")

plot(hc_euclidean, main = "Average Linkage (Euclidean)", xlab = "", sub = "")
plot(hc_manhattan, main = "Average Linkage (Manhattan)", xlab = "", sub = "")
```

5.  The Euclidean distance dendrogram is the best choice. When I split it into 4 clusters, the groups mostly match up, but some clusters are a mix

```{r}
dist_data <- dist(df_scaled, method = "euclidean")
hc_method <- hclust(dist_data, method = "complete")
plot(hc_method, main = "Complete Linkage Indvidual", xlab = "", sub = "")
rect.hclust(hc_method, k = 4, border = "blue")
cluster_assignments <- cutree(hc_method, k = 4)
table(cluster_assignments, df$Group)
```

6.  I chose Euclidean distance because it's ideal for continuous data and measures straight-line distance between points. It works well for clustering attitudes, providing clear separation between similar groups.

```{r}
# Perform hierarchical clustering with complete linkage
X_t <- t(X_scaled)
hc_attitudes_complete <- hclust(dist(X_t), method = "complete")
plot(hc_attitudes_complete, main = "Complete Linkage Attitude", xlab = "", sub = "")

rect.hclust(hc_attitudes_complete, k = 4, border = "green")
attitude_clusters <- cutree(hc_attitudes_complete, k = 4)
table(attitude_clusters)
```

7.  The dendrogram shows 4 clear clusters, each containing similar attitude variables. These clusters are well-separated, meaning the attitudes within each group are similar to each other. K-means just divides the data into 4 clusters, while the hierarchical clustering shows how these clusters form and how closely related the data points are. Hierarchical clustering gives more insight into how the clusters are connected.

```{r}
plot(hc_attitudes_complete, main = "Complete Linkage Attitudes", xlab = "", sub = "")
rect.hclust(hc_attitudes_complete, k = 4, border = "green")

attitude_clusters <- cutree(hc_attitudes_complete, k = 4)
table(attitude_clusters)
```

#### What teams need to do

1.  Review and combine individual results. Which methods for clustering the data points seemed to correspond best with the pre-established groups? What $K$ gave you the best results? What linkage? What distance metric?

    After reviewing all individual clustering results, we found that both K-means with K = 4 and hierarchical clustering using complete linkage with Euclidean distance performed best in terms of aligning with the original respondent groups. These methods most clearly separated the Indonesian respondents from the U.S.-based students, which was one of the key goals of the analysis. K-means was slightly easier to interpret and implement consistently across individuals, which led us to select it for our team’s final clustering. However, the hierarchical method also showed similar group structure when we used a cut at K = 4. Based on comparison tables and dendrograms, both models grouped IndoProf24 and IndoPost24 more distinctly than the other methods, giving us more confidence in their results.

    The print out below has the k means model and hierarchical clustering model that both performed similarly, though we ultimately used the k means model for question 3.

    ```{r}
    set.seed(44)
    km.out <- kmeans(df[, 2:50], 4, nstart = 20)
    # Output the cluster vs. actual group comparison
    table(km.out$cluster, df$Group)

    dist_euc <- dist(df_scaled, method = "euclidean")
    hc_complete <- hclust(dist_euc, method = "complete")
    plot(hc_complete, main = "Dendrogram - Complete Linkage (Individuals)", xlab = "", sub = "")
    rect.hclust(hc_complete, k = 4, border = "red") 
    cut_clusters <- cutree(hc_complete, k = 4)
    table(cut_clusters, df$Group) 
    ```

2.  Compare the results from Principal Component Analysis (PCA) with the results of clustering the attitudes. Which is more interpretable? What is your interpretation?

    Clustering provides a more interpretable framework for understanding attitudes because it assigns each observation to a specific group, allowing for a direct examination of how attitudes cluster together in practice. This discrete categorization makes it easier to identify coherent patterns and relationships among variables. In comparison, PCA transforms the original variables into linear combinations that maximize variance, which, while useful for dimensionality reduction, often results in components that are more abstract and harder to interpret substantively. The lack of clear group boundaries in PCA makes it less effective for identifying distinct attitude profiles compared to clustering methods.

    Below are the results from PCA:

    ```{r}
    pr.out <- prcomp(t(df[,2:50]), scale=TRUE)
    names(pr.out)

    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)
    pve

    par(mfrow = c(1, 2))
    plot(pve, xlab = "Principal Component",ylab = "Proportion of Variance Explained", ylim = c(0, 1),type = "b")
    plot(cumsum(pve), xlab = "Principal Component", ylab ="Cumulative Proportion of Variance Explained",ylim = c(0, 1), type = "b")


    ```

    ```{r}
    #So we should just using the first two principal components

    pr.out <- prcomp(t(df[,2:50]), scale=TRUE)


    pr.var<- pr.out$sdev^2

    pve<- pr.var / sum(pr.var)

    #using coefficents as measures of importance and relation
    cat("Which attitudes had high positive coefficents in PC1:","\n","\n")
    which((pr.out$x[,1])>7)
    cat("Which attitudes had high negative coefficents in PC1:","\n","\n")
    which(pr.out$x[,1] < -7)
    cat("\n","\n")
    cat("Which attitudes had high positive coefficents in PC2:","\n","\n")
    which((pr.out$x[,2])>3)
    cat("Which attitudes had high negative coefficents in PC2:","\n","\n")
    which(pr.out$x[,2] < -3)

    ```

    Using the PCA output for the first two principal components, which explained close to 40% of the variation in the data, we can see that some attitudes are scaled to be used higher in the principal components. Though the interpretability is difficult, we may attempt to interpret this as A6, A12, A16, A21, A25, A23 being related. Also, A1, A3, A4, A5, A9, A11, A15, A18, A19, A20 would be related. Next, A6, A7, A12, A13, A14, A16, A21 would be related. Finally, A2, A4, A5, A9, A10, and A18 would be related.

    Using PCA and clustering, we see that the fav aspect and freq aspect of each attitude are usually grouped together which makes sense. These aspects would probably be interpreted differently by respondents. Beyond that, it was difficult to find groupings that were similar between these methods. More observations may be needed to create reliable groupings.

3.  Create your team's final, best clustering of the data points. How well do the clusters correspond to the four groups?

    The clusters correspond fairly well to the four groups. Indopost24 and StatCollab are mostly grouped together, while IndoProf24 is separated but still mixed with some other groups. SLSp25 is more scattered across different clusters, indicating some overlap. Below is the best clustering of the data points for clustering the four groups:

    ```{r}
    set.seed(49)
    km.out <- kmeans(df[, 2:50], 4, nstart = 20)
    # Output the cluster vs. actual group comparison
    table(km.out$cluster, df$Group)
    ```

4.  Create your team's final, best clustering of the attitudes. What patterns do you see within the clusters of attitudes?

    Our best clustering of attitudes resulted from k=5 which successfully separated almost all of the attitudes. Some patters we saw were that A10, A9, and A24 were related. We also saw that attitudes A23, A25, and A21 were related. Next, we saw that A2, A8 and A22 were related. Beyond that, the clusters become larger and the patterns are harder to recognize in addition to the strength of their relationship being harder to verify. It does seem that attitudes with numbers near each other are usually clustered together which makes sense as most attitudes were asked about in sequences where they had similar emotional responses. Here is our best clustering of the attitudes:

    ```{r}
    X_t <- t(X_scaled)
    set.seed(42)
    kmeans_attitudes <- kmeans(X_t, centers = 5, nstart = 20)

    attitude_clusters <- data.frame(
      Attitude = colnames(X),
      Cluster = kmeans_attitudes$cluster
    )

    attitude_clusters <- attitude_clusters %>% arrange(Cluster)
    print(attitude_clusters)
    ```

5.  Write a paragraph about individual contributions. Each individual must comment on their contributions to the lab. Only individuals who contribute to the team section will get points for the team section. An example contribution could be: "I synthesized all of the individual results to find that such and such gave the best and most interpretable results. Then I recommended teammate X use this linkage and that distance measure for our team's cluster." Or, "I did the comparison of PCA with our clustering of the attitudes." Or, "I created our team's best clustering of the data points based on teammate Y's synthesis of our individual results." Or, "I took everyone's code and put it together into the qmd file and knitted it." Or, "I made the plot of our team's best clusters."

Josh: I created a PCA and compared it's interpretability to the clustering methods. I also contributed to the interpretation section of question 2. I also contributed to answering and verifying question 4

Chase : I created our teams final clustering of data points using K-means.

Will : I contributed to the interpretation of section 2 and general proof reading of the other questions as well

Rishi: I did question 1 of the team section and helped where I could for other parts of the team section.

#### Potentially useful code below

```{r include=F}
suppressPackageStartupMessages(library(tidyverse))
dat <- read_csv("Attitude.clean.csv")
dim(dat)

X <- dat[,-1] #Excluding the groups
dim(X)

# Creating distance measures
cor_spearman <- cor(t(X), method = "spearman") # Transpose so the rows are the attitude variables
dist_spearman <- as.dist(1 - cor_spearman)

cor_pearson <- cor(t(X), method = "pearson") 
dist_pearson <- as.dist(1 - cor_pearson)

# Using Manhattan or Euclidean distance
dist_manhattan <- dist(X, method = "manhattan") #Manhattan distance counts 
# the "blocks" between data points as if you were walking along the streets of 
# Manhattan (i.e., you can't cut diagonally through a city block)
dist_euclidean <- dist(X, method = "euclidean") #Euclidean distance is the 
# diagonal "shortest path" between points A and B.

# Using hclust
hc.complete <- hclust(dist_spearman, method = "complete")
cutree(hc.complete, k=4)

# Kmeans


# Doing PCA
pr.out <- prcomp(X,scale=F) #The attitudes are basically already on the same scale, so scaling the attitudes would confuse things, though A1.freq is on a 5-point scale and A1.fav is on a 6-point scale

# Find means of the four groups
SL.means <- colMeans(dat[dat[,1]=="SLSp25",2:51])
StatCollab.means <- colMeans(dat[dat[,1]=="StatCollab",2:51])
IndoProf.means <- colMeans(dat[dat[,1]=="IndoProf24",2:51])
IndoPost.means <- colMeans(dat[dat[,1]=="IndoPost24",2:51])

# Then transform the group centroids into the Principal Component space
C.matrix <- rbind(StatCollab.means,SL.means,IndoProf.means,IndoPost.means)

#Transform C.matrix into scores using the rotation matrix of the first two principal components
W2 <- pr.out$rotation[,1:2]
Group.scores <- C.matrix %*% W2
Group.scores <- mutate(as.data.frame(Group.scores),groups=rownames(Group.scores))
ggplot(data=Group.scores, aes(x=PC1,y=PC2,col=groups)) +
  geom_point()

# Comparing found clusters with groups
# What percentage of each cluster is each group?
table(km.out$cluster,dat$Group) #can transform this into percentages if necessary

# We can do the same with clusters found from hiearchical clustering
table(cutree(hc.complete,k=4),dat$Group) #These clusters aren't very good.



```
