---
title: "Lab3"
author: "Mountain Dewds"
format: pdf
editor: visual
date: 2025-03-14
---

## Lab3: Support Vector Machines

**Due: 11:59PM Sunday, March 23 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually fit a support vector machine, describe what it is doing, and report its misclassification rate, sensitivity, and specificity.
2.  You will interpret this model and make a prediction in your individual section.
3.  As a team, you will create a support vector machine (SVM) and compete against other teams for the best test performance. You will also create a team plot that summarizes the performance of the individual SVMs and your team's SVM.

### Instructions for Lab3

In Lab0 (and Lab1), the professor created a generating model for Y variables taking on 0 or 1 values based on the X1 and X2 inputs. You came up with the backstory for what Y, X1, and X2 were and why it was important to use X1 and X2 to predict Y.

In this lab, you will apply support vector machines to continue your analyses for how to best predict Y from X1 and X2. You should continue to use the Q1 qualitative context you developed in Lab0 or Lab1 (or you can develop a new one if you want).

Each individual will fit at least one SVM on a training dataset and then evaluate how well the SVM classifies Y based on a test dataset. Individuals will make a prediction given (x1, x2) and then interpret their prediction and make a recommendation for action. Unlike in Lab0 and Lab1, you will not describe the whole workflow of Q1, Q2, and Q3 for this "project". You will report on the misclassification rate, sensitivity, and specificity for your SVM and make a prediction given (x1, x2). You will comment on how the prediction differs from your prediction in Lab1 and, given this new prediction, describe what actions (Q3) you recommend given your Q2 results.

#### Generating Model

We have a logistic regression generating model. Given $x_1 \in [0,1]$ and $x_2 \in [0,1], Y \sim Ber(p)$, where $p$ is related to $x_1$ and $x_2$ through the logistic link function: $\log(\frac{p}{(1-p)}) = x_1 -2x_2 -2{x_1}^{2} + {x_2}^{2} + 3{x_1}{x_2} +4{x_1}{x_2}^2 -3{x_1}^{2}x_2$, where $\log$ is the natural log, sometimes written as $\ln$.

The code for this is below.

```{r}
library(class)
suppressPackageStartupMessages(library(tidyverse))
```

```{r}
#Generative model
set.seed(200) #setting a random seed so that we can
#reproduce everything exactly if we want to

generate_y <- function(x1,x2) { #two input parameters to generate the output y
  logit <- x1 -2*x2 -2*x1^2 + x2^2 + 3*x1*x2 +4*x1*x2^2 -3*x1^2*x2
  p <- exp(logit)/(1+exp(logit)) #apply the inverse logit function
  y <- rbinom(1,1,p) #y becomes a 0 (with prob 1-p) or a 1 with probability p
}
```

#### Training dataset

We are going to use our generating model to create a training dataset of 1000 predictors (x1, x2), and then 1000 outcomes. Then we plot all three variables to see what our training data looks like.

```{r}
# Generate a training dataset with 1000 points
set.seed(314)
n = 1000
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 1000 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #How many 0's and 1's were predicted? When n is very large
# about 51.5% of the Y's should be 1's. 

training <- cbind(X1,X2,Y) #combining all of my variables into a training dataset
ggplot(data=training, aes(x=X1, y=X2, color=Y)) +
  geom_point()
```

How well will various SVMs predict Y given new x1 and x2 values?

#### Test datasets

Each individual will generate a test set of 100 predictors (x1, x2) and outcomes (y) that we will use as our "ground truth". The test dataset should be the same for each team member. Use seed=323.

```{r}
# Create the training dataset as above using seed=314
# Create a testing dataset using seed=323

set.seed(323)
n = 100

X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 1000 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #How many 0's and 1's were predicted? When n is very large
# about 51.5% of the Y's should be 1's. 

testing <- cbind(X1,X2,Y) #combining all of my variables into a training dataset
ggplot(data=training, aes(x=X1, y=X2, color=Y)) +
  geom_point()
```

#### What individuals need to do (20 pts)

1.  Given the training set (seed=314), fit a ***different*** SVM for each team member. Choose from the options below:
    1.  linear SVM (this will be a support vector classifier, i.e., a flat hyperplane \[actually just a line\])

    2.  SVM with an interaction between X1 and X2

    3.  SVM with degree 2 polynomials

    4.  SVM with degree 3 polynomials
2.  Use cross validation to choose your tuning parameters.
3.  Describe what the SVM is doing, i.e., how it classifies points.
4.  Calculate the sensitivity, specificity, and overall error rate (misclassification rate) for your SVM given the test set of 100 data points.
5.  Make a prediction for a new point (x1, x2) = (0.25, 0.25) or (0.25, 0.75) or (0.75, 0.25) or (0.75, 0.75) for each fitted model. Each individual will have a different point for their predictions. Use the same one you used for Lab1
6.  Comment on how the prediction differs from your prediction(s) in Lab1 and, given this new prediction, describe what actions (Q3) you recommend given your Q2 results.

# JOSH Individual

1.  I chose to fit an SVM with degree 3 polynomials

    ```{r}

    ```

2.  Below I used cross validation to select my tuning parameters

    WARNING: This code took very long (the tuning portion especially). I noted the selected parameters within the code, so you do not need to run the code again to fit the best model.

```{r}

training_df<-data.frame(training)
training_df$Y<-as.factor(training_df$Y)
head(training_df)


library(e1071)



#I used to built in function to do 10 fold cross validation for the SVM




# Using this summary, we saw the error was the least
# with a C around 0.01, so C values between 0.001 and 0.1 were investiated
# as well as gammas around 0.1 and 1

possible_costs<- c(0.001,0.01,0.1,1)
possible_gamma<-c(0.1,1,10)
possible_coef0<-c(0,0.25,0.5,0.75)

#Again 10 fold cross validation was done for the SVM with these possible vals
 tune.out <- tune(svm, Y ~ ., data = training_df, kernel = "polynomial", degree=3, ranges = list(cost=possible_costs,gamma=possible_gamma,coef0=possible_coef0))
 
summary(tune.out)

#coef0= 0 always within this summary so it was not repeated

#cost of about 0.1 and gamma=1 seemed to perform best

#Tuned one more time around these parameters

possible_costs<- seq(0.1,1,0.025)
possible_gamma<- c(0.05,1,1.5)

tune.out <- tune(svm, Y ~ ., data = training_df, kernel = "polynomial", degree=3, ranges = list(cost=possible_costs,gamma=possible_gamma,coef0=0))
 
summary(tune.out)
 
#Using built in functions, I outputted the best model with cost of 0.125 and gamma=1
bestmod <- tune.out$best.model
summary(bestmod)

```

\

3.  The SVM is classifying points by using a polynomial kernel of degree 3 to fit the predictor space of X1 and X2 with a polynomial function and marginal error lines (where the support vectors lie). The SVM classifies each point depending on what side of the classification line (this "line" looks like a polynomial of degree 3) each observation lies. See the plot below:

    ```{r}
    plot(bestmod,training_df)
    ```

4.  Below I calculated sensitivity, specificity, and overall error rate given the testing data set.

    ```{r}
    testing_df<-data.frame(testing)
    testing_df$Y<-as.factor(testing_df$Y)

    predictions<-predict(bestmod,testing_df)

    confmatrix<-table(predictions,testing_df$Y)

    sensitivity<-confmatrix[1,1]/sum(confmatrix[,1])
    specificity<-confmatrix[2,2]/sum(confmatrix[,2])
    misclassification_rate<- sum(predictions != testing_df$Y)/length(predictions)

    cat("Sensitivity is:","\n")
    sensitivity

    cat("specificity is:","\n")
    specificity

    cat("Misclassification Rate is:","\n")
    misclassification_rate


    ```

5.  I used the point (0.25,0.25)

    ```{r}
    df<-data.frame("X1"=0.25,"X2"=0.25)

    X1_X2_pred<-predict(bestmod,newdata=df)
    data.frame(X1_X2_pred)
    ```

6.  This prediction is 0 and when compared to my prediction is lab 1 for the same point (which was also Y=0). So my prediction is the same using this model.

    Just to reiterate from lab 1, I chose to interpret the variables as such: Y is classifying a house price as above the median sale price in CO. X1 is average yearly sun exposure to the house (re-scaled to have the range of X1 in the data frame. X2 is the time of mail deliveries to the house (re-scaled to have the range of X2 in the data frame.)

    The Q2 portion of this project is fitting the data with different SVM models (in my case the polynomial of degree 3), using our best model and predicting on a new house with values of (0.25,0.25). Using the best model, I then predicted that a house with values of (0.25,0.25) will sell below the median sale price of houses in CO (Y=0). So, if we were to give a recommendation to this company based on the SVM model with polynomial degree 3, we would recommend that they not buy the house with values of (0.25,0.25) because it will likely not sell highly. The misclassification rate is still very high, so this model is likely not a great one to base recommendations on.

# Chase's Section 
```{r}
library(tidyverse)
library(e1071)

generate_y <- function(x1, x2) {
  logit <- x1 - 2*x2 - 2*x1^2 + x2^2 + 3*x1*x2 + 4*x1*x2^2 - 3*x1^2*x2
  p <- exp(logit) / (1 + exp(logit)) 
  y <- rbinom(1, 1, p) 
  return(y)
}

set.seed(314)
n_train <- 1000
X1_train <- runif(n_train, 0, 1)
X2_train <- runif(n_train, 0, 1)
Y_train <- sapply(1:n_train, function(i) generate_y(X1_train[i], X2_train[i]))
training_data <- data.frame(X1 = X1_train, X2 = X2_train, Y = as.factor(Y_train))

set.seed(323)
n_test <- 100
X1_test <- runif(n_test, 0, 1)
X2_test <- runif(n_test, 0, 1)
Y_test <- sapply(1:n_test, function(i) generate_y(X1_test[i], X2_test[i]))
test_data <- data.frame(X1 = X1_test, X2 = X2_test, Y = as.factor(Y_test))
```
```{r}
training_data$X1_X2 <- training_data$X1 * training_data$X2
test_data$X1_X2 <- test_data$X1 * test_data$X2

# Tune SVM with 10-fold cross-validation
set.seed(314)
tuned_svm <- tune(svm, Y ~ X1 + X2 + X1_X2, data = training_data,
                   kernel = "radial", 
                   ranges = list(cost = 10^seq(-2, 2, by = 1), gamma = 10^seq(-2, 2, by = 1)))


best_svm <- tuned_svm$best.model
```
3. This SVM model creates a binary class (Y=0 and Y=1) by using X1, X2, and their interaction. We use a non linear kernel of (X1, X2, X1*X2) to then find the optimal hyperplane. The SVM classifies each observation depending on which side of the hyperplane it falls on. 

4. 
```{r}
preds <- predict(best_svm, test_data)
conf_matrix <- table(Predicted = preds, Actual = test_data$Y)

sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[, 2])  # TP / (TP + FN)
specificity <- conf_matrix[1, 1] / sum(conf_matrix[, 1])  # TN / (TN + FP)
error_rate <- sum(preds != test_data$Y) / length(test_data$Y)  

cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("Error Rate:", error_rate, "\n")
```
5. I used point (0.75,0.25)
```{r}
data<-data.frame("X1"=0.75,"X2"=0.25, X1X2 = 0.75 * 0.25)
X1_X2_pred<-predict(bestmod,newdata=df)
data.frame(X1_X2_pred)
```
6. My prediction of 0 is the same as my previous prediction in Lab 1. In lab 1 I decided to predict whether a student will pass(Y=1) or fail(Y=1) with my X1 being study hours and X2 being previous exam scores. 

The Q2 of this lab would be fitting a SVM model with an interaction between X1 and X2. We then would use our best model to predict if a student will pass or fail with X1 = 0.75 and X2=0.25. Based on my prediction I would highly recommend to this student to increase their hours studied. 


# Will's Individual

```{r}
library(class)
suppressPackageStartupMessages(library(tidyverse))
```


```{r}
set.seed(200)
generate_y <- function(x1,x2) {
logit <- x1 -2*x2 -2*x1^2 + x2^2 + 3*x1*x2 +4*x1*x2^2 -3*x1^2*x2
p <- exp(logit)/(1+exp(logit))
y <- rbinom(1,1,p)
}

set.seed(314)
n = 1000
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)
Y <- rep(0,n) 
for (i in 1:n) {
Y[i] <- generate_y(X1[i],X2[i])
}
sum(Y)
training <- as.data.frame(cbind(X1,X2,Y))
```

```{r}
set.seed(323)
n = 100
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)
Y <- rep(0,n) 
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y)
testing <- as.data.frame(cbind(X1,X2,Y))
```

## 1. and 2. fitting model and using cv to determine best parameters
This is for an SVM based on degree 2 polynomials
```{r}
training_df <- data.frame(training)
training_df$Y <- as.factor(training_df$Y)
head(training_df)
library(e1071)
cost_values <- c(0.001, 0.01, 0.1, 1, 1)
gamma_values <- c(0.1, 1, 10)
coef0_values <- c(0, 0.25, 0.5, 0.75)
svm_tuning <- tune(svm, Y ~ ., data = training_df, kernel = "polynomial", degree = 2, 
                   ranges = list(cost = cost_values, gamma = gamma_values, coef0 = coef0_values))
summary(svm_tuning)
cost_range <- seq(0.1, 1.1, 0.025)
gamma_range <- c(0.05, 1, 1.5)
svm_tuning <- tune(svm, Y ~ ., data = training_df, kernel = "polynomial", degree = 2, 
                   ranges = list(cost = cost_range, gamma = gamma_range, coef0 = 0))
summary(svm_tuning)
best_svm_model <- svm_tuning$best.model
summary(best_svm_model)

```
## 3.
```{r}
    plot(best_svm_model,training_df)
```
The SVM is performing classification using a polynomial kernel of degree 2, mapping the predictor space of X1 and X2 into a higher-dimensional space. This allows it to fit a nonlinear decision boundary that separates the classes while adding a margin. The support vectors lie along the margin, influencing the placement of the decision boundary. Each observation is classified based on which side of this boundary it falls on—where the boundary itself follows a quadratic curve due to the degree-2 polynomial kernel. 

## 4.

    ```{r}
    testing_df<-data.frame(testing)
    testing_df$Y<-as.factor(testing_df$Y)
    predictions<-predict(best_svm_model,testing_df)
    confmatrix<-table(predictions,testing_df$Y)
    sen<-confmatrix[1,1]/sum(confmatrix[,1])
    spec<-confmatrix[2,2]/sum(confmatrix[,2])
    mis<- sum(predictions != testing_df$Y)/length(predictions)
    cat("Sensitivity","\n")
    sen
    cat("Specificity","\n")
    spec
    cat("Misclassification Rate","\n")
    mis
    ```

## 5.

```{r}
data<-data.frame("X1"=0.25,"X2"=0.75, Y = 0.75 * 0.25)
Y_pred<-predict(best_svm_model,newdata=data)
data.frame(Y_pred)
```
## 6.

My original model for lab one predicted Y=1 for this point, while this svm model predicted Y=0. My lab 1 Q1 Q3 scenario was predicting whether or not someone would default on their loan given late payment history and credit score. This changes my prediction from lab 1 and now predicts that the individual described by this data point would not default on their loan. Based on this new classification, a company should offer this customer a loan. That being said, the misclassification rate was still 0.4, so this should not be the only way that this a company should decide on offering or declining a loan.


# MOUNTAIN DEWDS TEAM SECTION

1.  Create a new team SVM that predicts the test set better than any individual SVM.

    Below is our team SVM:

    ```{r}
    tune.out <- tune(svm, Y~., data = training_df, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000),gamma = c(0.5,1,2,3,4)))

    summary(tune.out)
    #cost around 0.1 to 1 should be investigated, gamma around 1-3 should be investigated

    tune.out <- tune(svm, Y~., data = training_df, kernel = "radial", ranges = list(cost = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),gamma = c(1,2,3)))

    teammod<-tune.out$best.model

    #Best is still cost=0.9,gamma=1
    tune.out

    predictions<-predict(teammod,testing_df)

    confmatrix<-table(predictions,testing_df$Y)

    sensitivity<-confmatrix[1,1]/sum(confmatrix[,1])
    specificity<-confmatrix[2,2]/sum(confmatrix[,2])
    misclassification_rate<- sum(predictions != testing_df$Y)/length(predictions)

    cat("Sensitivity is:","\n")
    sensitivity

    cat("specificity is:","\n")
    specificity

    cat("Misclassification Rate is:","\n")
    misclassification_rate
    ```

    The best model used a cost of 0.9 and gamma 1.

2.  Describe what the SVM is doing, i.e., how it classifies points.

3.  Make a plot that summarizes the performance of the individual SVMs and your team's SVM.

    Below is a plot with the sensitivity, specificity, and overall misclassification rate of each SVM:

    ```{r}

    ```

4.  Provide R code that can easily be run in class to test the performance of your team's SVM (on a new test set). The code must accept a test dataset of 100 X and y values and output the misclassification rate, sensitivity, and specificity of the SVM. More specifically, write the code so that the only thing that needs to change is the random seed. Add some aspect of your team name to your code so we can distinguish between different teams' functions. Email your .qmd file to the professor. (Upload a pdf to Canvas.) The team with the best classifier on the test set the professor will generate in class (using a different seed) will get 10 bonus points. Second place will get 5 bonus points. Third place will get 3 bonus points. Fourth place will get 2 bonus points. Fifth place will get 1 bonus points. Only teams whose code runs (in class) on the first try will get any bonus points.

    ```{r}
    # MOUNTAIN DEWDS



    # SET SEED
    set.seed(323)

    #TEST DATA SET
    n = 100
    X1 <- runif(n,0,1)
    X2 <- runif(n,0,1)
    Y <- rep(0,n)
    for (i in 1:n) {
      Y[i] <- generate_y(X1[i],X2[i])
    }

    #creating data frame
    testing <- cbind(X1,X2,Y)
    testing<- data.frame(testing)
    testing$Y<- as.factor(testing$Y)
    ## Test set ready



    ##Creating Model with test set (cost = , gamma =)
    MD_mod<- svm(Y ~ ., data = testing, kernel = "radial", gamma = 1, cost = 0.9)

    #Predictions and rates
    predictions<-predict(MD_mod,testing)
    confmatrix<-table(predictions,testing$Y)
    sensitivity<-confmatrix[1,1]/sum(confmatrix[,1])
    specificity<-confmatrix[2,2]/sum(confmatrix[,2])
    misclassification_rate<- sum(predictions != testing$Y)/length(predictions)
    cat("Sensitivity is:","\n")
    sensitivity
    cat("specificity is:","\n")
    specificity
    cat("Misclassification Rate is:","\n")
    misclassification_rate

    ```

5.  In the team section of this lab, write a paragraph about individual contributions. Each individual must comment on their contributions to the lab. Only individuals who contribute to the team section will get points for the team section.

#### 
