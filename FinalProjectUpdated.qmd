---
title: "Final Project"
author: "Mountain Dewds"
format: pdf
editor: visual
date: 2025-04-18
---

## Final Project: Applying Various Methods to Answer Questions of Interest

**Due: 11:59PM Wednesday, April 30 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually fit **at least three statistical learning models** to answer your individual question of interest based on your team's chosen data set. You will individually explain what the relevant problem context is (Q1), what methods you used and their results (Q2), how those results answer the question of interest (Q3), and what implications (recommendations, next step actions, etc.) the answers have (Q3). In addition, you will comment on some ethical aspect of the problem.
2.  **Each individual must use at least one unique method**, i.e., not a method used by any of your teammates. For example, if team member 1 fit a cubic spline, BART, and a feedforward neural network to answer their individual research question and member 2 used a cubic spline, BART, and random forest, and member 3 used BART, natural spline, and lasso, then member 4 could not use a feedforward NN or a random forest or both natural spline and lasso.
3.  Teams will propose and answer a team research question that should incorporate the individual contributions. In other words, teams will combine findings from the individual components when answering their team question. Each individual must contribute to the team part and state what their contribution was. No stated contribution by individuals means no team points for that individual.
4.  Team answers should follow the Q1-Q2-Q3 workflow, including a reflection about an ethical principal used or some other ethical aspect of the team solution. Teams must also produce a plot that helps to tell the narrative of the data. (Individuals should also include plots in their individual parts, but it's not strictly required.)

#### What teams need to do first

1.  Choose a dataset that has enough interesting predictor X variables so that you can have four individual sub-research questions and one overall team research question. You are encouraged to gain practice doing unsupervised learning (PCA, K-means or hierarchical clustering) if that makes sense for your data set and your research questions.

    ```{r}
    #Loading the dataset
    library(datasets)
    head(airquality)


    #Cleaning the dataset

    team_data_frame<-airquality

    team_data_frame$Month<- as.factor(team_data_frame$Month)
    team_data_frame$Day<- as.factor(team_data_frame$Day)




    ```

2.  Brainstorm various research questions. Each individual needs their own sub-research question to answer. Think about how the answers to the sub-research questions could inform the answer to the overall team research question.

    Team research question: Our team research question is "what model can be used to best predict wind in miles per hour in New York?"

    Individual research questions:

    Josh: Can we reliably model wind speeds just using the temperature and what model would be best to do this? It seems that temperature could be a good predictor as temperature and wind speeds may be seasonal and somewhat correlated. For this question I will use generalized linear regression, a smoothing spline, polynomial regression, and local regression (polynomial ended up just acting as linear after tuning)

    Will: How many principle components would create the most efficient and accurate model for predicting wind speeds in NYC?

    Chase: Which is the strongest predictor of wind speed: temperature, solar radiation, or ozone levels? I will use three models to answer this question, multiple linear regression, lasso, and random forest.

    Rishi: Can we predict how much ozone is in the air on a given day just by looking at the weather?

#### What individuals need to do

1.  Given your team's data set and your sub-research question, fit at least three statistical learning models. For each model fit, you should also play around with different tuning parameters, likely using CV.
2.  Explain Q1, Q2, and Q3.
3.  Comment on any ethical implications of this work.
4.  Highly recommended: Create visualizations of your models or just of the best model.

## JOSH INDIVIDUAL

1.  Given your team's data set and your sub-research question, fit at least three statistical learning models. For each model fit, you should also play around with different tuning parameters, likely using CV.

    My sub-research question is: can we reliably model wind speeds just using the temperature and what model would be best to do this? It seems that temperature could be a good predictor as temperature and wind speeds may be seasonal and somewhat correlated. For this question I will use generalized linear regression, a smoothing spline, polynomial regression, and local regression (polynomial ended up just acting as linear after tuning)

    First I created my own data frame to investigate this research question:

    ```{r}
    set.seed(44)
    library(ggplot2)
    josh_research_df<-data.frame("Wind"=team_data_frame$Wind,"Temp"=team_data_frame$Temp)

    #Creating training and validation set
    #Used 15% of data for validation

    index<-sample(1:nrow(josh_research_df),ceiling(0.15*nrow(josh_research_df)))

    js_validation_data<-josh_research_df[index,]
    js_training_data<-josh_research_df[-index,]
    ```

    Generalized Linear Regression

    ```{r}
    set.seed(44)
    lmod<-lm(Wind~Temp, data=js_training_data)

    summary(lmod)

    wind_pred<-predict(lmod,newdata=js_validation_data)

    js_validation_data$LMOD_Predictions<-wind_pred

    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=LMOD_Predictions),color="green")

    MSPE<-mean((js_validation_data$Wind-wind_pred)^2)
    MSPE
    ```

    Smoothing Spline

    ```{r}

    library(splines)
    library(dplyr)

    set.seed(44)
    ##K-fold CV
    k=10
    n=nrow(js_training_data)
    width=n/k
    test_indexes<-sample(1:n,replace=FALSE,n)
    natural_MSE<-matrix(NA,ncol=6,nrow=10)
    for (i in 1:k){
    test<-data.frame("Wind"=js_training_data[(test_indexes[as.integer(width*(i-1) ):as.integer(width*i)]),1],"Temp"=js_training_data[test_indexes[as.integer(width*(i-1) ):as.integer(width*i)],2])
    test_wind<-data.frame("Temp"=test$Temp)
    train_index<-setdiff(test_indexes,test_indexes[(width*(i-1) ):(width*i)])
    train<-data.frame("Wind"=js_training_data[train_index,1],"Temp"=js_training_data[train_index,2])



    for (j in 1:6){
    degrees_of_freedom=j
    natural_spline<-lm(data=js_training_data,Wind~ns(Temp, df=degrees_of_freedom))
    natural_pred<-predict(natural_spline,newdata=test_wind)
    natural_MSE[i,j]<-mean((test$Wind-natural_pred)^2)
    }
    }

    which(colMeans(natural_MSE)==min(colMeans(natural_MSE)))

    degrees_of_freedom=6
    natural_spline<-lm(data=js_training_data,Wind~ns(Temp, df=degrees_of_freedom))
    natural_pred<-predict(natural_spline,newdata=js_validation_data)
    MSPE<-mean((js_validation_data$Wind-natural_pred)^2)
    MSPE

    js_validation_data$natural_predictions<-natural_pred
    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=natural_predictions),color="yellow")


    ```

    Polynomial Regression: For the polynomial regression, I used k=10 as it divided into the number of rows easily. Note that this ended up just being linear regression, so I fit one more model.

    ```{r}
    set.seed(44)
    k=10
    n=nrow(js_training_data)
    width=n/k
    test_indexes<-sample(1:n,replace=FALSE,n)
    poly_mse<-matrix(NA,ncol=5,nrow=10)
    for (i in 1:k){
    test<-data.frame("Wind"=js_training_data[(test_indexes[as.integer(width*(i-1) ):as.integer(width*i)]),1],"Temp"=js_training_data[test_indexes[as.integer(width*(i-1) ):as.integer(width*i)],2])
    test_wind<-data.frame("Temp"=test$Temp)
    train_index<-setdiff(test_indexes,test_indexes[(width*(i-1) ):(width*i)])
    train<-data.frame("Wind"=js_training_data[train_index,1],"Temp"=js_training_data[train_index,2])

    #initializing MSE over each fold

    for (j in 1:5){
    poly_degree=j
    polyfit<- lm(data=train, Wind~ poly(Temp,poly_degree))
    poly_pred<-predict(polyfit,newdata=test_wind)
    poly_mse[i,j]<-mean((test$Wind-poly_pred)^2)
    }

    }

    #finding best average MSE for polynomial degree

    which(colMeans(poly_mse)==min(colMeans(poly_mse)))

    #This still results in a polynomial of the 1st degree to be the best. So we should use basically a linear regression.


    poly_degree=1
    polyfit<- lm(data=js_training_data, Wind~ poly(Temp,poly_degree))
    poly_pred<-predict(polyfit,newdata=js_validation_data)
    #Mean squared prediction error with validation Data
    MSPE<-mean((js_validation_data$Wind-poly_pred)^2)
    MSPE

    js_validation_data$poly_predictions<-poly_pred

    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=poly_predictions),color="green")

    ```

    Local Regression:

    ```{r}
    set.seed(44)
    k=10
    n=nrow(js_training_data)
    width=n/k
    test_indexes<-sample(1:n,replace=FALSE,n)
    local_mse<-matrix(NA,ncol=5,nrow=10)
    for (i in 1:k){
    test<-data.frame("Wind"=js_training_data[(test_indexes[(width*(i-1) ):(width*i)]),1],"Temp"=js_training_data[test_indexes[(width*(i-1) ):(width*i)],2])
    test_wind<-data.frame("Temp"=test$Temp)
    train_index<-setdiff(test_indexes,test_indexes[(width*(i-1) ):(width*i)])
    train<-data.frame("Wind"=js_training_data[train_index,1],"Temp"=js_training_data[train_index,2])

    j_tune<-seq(0.1,0.5, by=0.1)
    for (j in j_tune){
    span_val=j
    local_regression_fit<-loess(Wind~Temp,data=train, span=span_val)
    local_pred<-predict(local_regression_fit,newdata=test_wind)
    local_mse[i,j*10]<-mean((test$Wind-local_pred)^2)
    }
    }

    which(colMeans(local_mse[c(2,3,5,6,7,8,9,10),])==min(colMeans(local_mse[c(2,3,5,6,7,8,9,10),])))


    #Indicating errors with
    span_val=0.5
    local_regression_fit<-loess(Wind~Temp,data=js_training_data, span=span_val)
    local_pred<-predict(local_regression_fit,newdata=js_validation_data)

    #Mean squared prediction error with validation Data
    MSPE<-mean((js_validation_data$Wind-local_pred)^2)
    MSPE

    js_validation_data$loess_predictions<-local_pred

    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=loess_predictions),color="red")

    ```

    So out of these 4 methods of fitting wind to temperature, we were not able to reliably find a high accuracy model and surprisingly, the linear model performed the best!

2.  Explain Q1, Q2, and Q3.

    1.  The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data). Daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.

        -   `Ozone`: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

        -   `Solar.R`: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park

        -   `Wind`: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

        -   `Temp`: Maximum daily temperature in degrees Fahrenheit at LaGuardia Airport.

    Using this data, it seemed reasonable to think that Wind may be able to be completely modeled by Temperature as a predictor because wind speeds usually depend on temperature and time of year, but temperature and time of year are likely collinear. After considering a correlation plot, a linear model, splines, and a neural net seemed like all possible options for models to predict wind speeds with temperature. Each model was fit and tuned (it turns out that a tuned polynomial regression was equivalent to a linear regression model) and their MSEs were compared. Using the MSE's we saw that the linear regression actually had the lowest mean squared prediction error of about 10.1. This indicates that Wind and Temperature likely have a linear relationship, but the R\^2 value is extremely low (about 1.8) so there could be a better model, though I could not find it.

    Now considering the applications of this modeling, I would recommend to the interested parties in modeling wind speeds solely based off of temperature to investigate different types of models than the ones investigated here. These models had high MSEs and would not reliably predict wind speeds using temperature except to show that wind speeds are higher on average for lower temperatures.

3.  Comment on any ethical implications of this work.

    The ethical implications of this work for the individual assignment are hard to pin down but ethical issues could arise. These include giving wind forecasting to airplanes. If these models were used to give wind forecasts for airplanes, their predictions seem to be fairly unreliable. This could lead to catastrophic events where planes that should not have taken off or taken a different route to their destination failed to do so because their wind predictions were incorrect. Wind speed predictions may also be used for ski areas. If the winds are predicted to stay low, when they in fact are actually high, then a ski lift could run with passengers during very dangerous conditions. Other ethical implications could result from this work, but these are some examples, though not exhaustive.

4.  Highly recommended: Create visualizations of your models or just of the best model.

    Below is a final visualization of all the models together:

    ```{r}
    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=natural_predictions),color="blue")+geom_line(aes(x=Temp,y=loess_predictions),color="red")+geom_line(aes(x=Temp,y=poly_predictions),color="green")+geom_line(aes(x=Temp,y=natural_predictions),color="yellow")
    ```

## CHASE INDIVIDUAL

```{r}
set.seed(44)

chase_research_df <- data.frame("Wind" = team_data_frame$Wind,
                                 "Temp" = team_data_frame$Temp,
                                 "Solar.R" = team_data_frame$Solar.R,
                                 "Ozone" = team_data_frame$Ozone)

# Remove rows with missing values
chase_research_df <- na.omit(chase_research_df)

# Split the data into training (85%) and validation (15%) sets
index <- sample(1:nrow(chase_research_df), ceiling(0.85 * nrow(chase_research_df)))

training_data <- chase_research_df[index, ]
validation_data <- chase_research_df[-index, ]


```

Multiple Linear Regression

```{r}
library(caret)
library(datasets)
team_data_frame <- na.omit(team_data_frame)

set.seed(44)
index <- sample(1:nrow(team_data_frame), ceiling(0.85 * nrow(team_data_frame)))
training_data <- team_data_frame[index, ]
validation_data <- team_data_frame[-index, ]

# 10 fold CV
train_control <- trainControl(method = "cv", number = 10)
lm_cv_model <- train(Wind ~ Temp + Solar.R + Ozone, data = training_data,
                     method = "lm", trControl = train_control)
print(lm_cv_model)
lm_predictions <- predict(lm_cv_model, newdata = validation_data)
validation_data$LM_Predictions <- lm_predictions


lm_coef <- coef(lm_cv_model$finalModel)[-1]   
lm_coef_df <- data.frame(Predictor = names(lm_coef), Coefficient = lm_coef)

ggplot(lm_coef_df, aes(x = Predictor, y = Coefficient)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Multiple Linear Regression Coefficients", x = "Predictor", y = "Coefficient Value") +
  theme_minimal()

# MSPE
lm_mspe <- mean((validation_data$Wind - lm_predictions)^2)
cat("Multiple Linear Regression MSPE: ", lm_mspe, "\n")

```

Lasso

```{r}
library(glmnet)
library(caret)

training_data <- na.omit(training_data)
validation_data <- na.omit(validation_data)

X_train <- as.matrix(training_data[, c("Temp", "Solar.R", "Ozone")])
y_train <- training_data$Wind
X_val <- as.matrix(validation_data[, c("Temp", "Solar.R", "Ozone")])
y_val <- validation_data$Wind

# Fit Lasso using cv
lasso_model <- cv.glmnet(X_train, y_train, alpha = 1)

# Best lambda from cross-validation
best_lambda_lasso <- lasso_model$lambda.min
cat("Best lambda for Lasso: ", best_lambda_lasso, "\n")
lasso_predictions <- predict(lasso_model, newx = X_val, s = "lambda.min")
lasso_predictions <- as.vector(lasso_predictions)


validation_data$Lasso_Predictions <- lasso_predictions

# MSPE
lasso_mspe <- mean((validation_data$Wind - lasso_predictions)^2)
cat("Lasso Regression MSPE: ", lasso_mspe, "\n")


lasso_coef <- coef(lasso_model, s = "lambda.min")[-1]  # Remove intercept
print(lasso_coef)
lasso_coef_df <- data.frame(Predictor = c("Temp", "Solar.R", "Ozone"), 
                            Coefficient = c(-0.04763713, 0, -0.04848467))

# Plot the coefficients 
ggplot(lasso_coef_df, aes(x = reorder(Predictor, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity", fill = ifelse(lasso_coef_df$Coefficient < 0, "red", "blue")) + 
  coord_flip() +  
  labs(title = "Lasso Regression Coefficients for Wind Speed Prediction",
       x = "Predictor", y = "Coefficient Value") +
  theme_minimal() +
  theme(axis.text = element_text(size = 12))  
```

Random Forests

```{r}
library(randomForest)
library(caret)
# 10 fold CV
train_control <- trainControl(method = "cv", number = 10)

rf_cv_model <- train(Wind ~ Temp + Solar.R + Ozone, data = training_data,
                     method = "rf", trControl = train_control, tuneLength = 10)
print(rf_cv_model)

rf_predictions <- predict(rf_cv_model, newdata = validation_data)
validation_data$RF_Predictions <- rf_predictions


rf_importance <- importance(rf_cv_model$finalModel)
rf_importance_df <- data.frame(Predictor = rownames(rf_importance), Importance = rf_importance[, 1])

# MSPE
rf_mspe <- mean((validation_data$Wind - rf_predictions)^2)
cat("Random Forest MSPE: ", rf_mspe, "\n")
# Plot
ggplot(rf_importance_df, aes(x = reorder(Predictor, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "purple") +
  coord_flip() +
  labs(title = "Random Forest Feature Importance", x = "Predictor", y = "Importance") +
  theme_minimal()
```

Q1, Q2, & Q3

Q1: The data comes from the New York State Department of Conservation and the National Weather Service, with daily measurements from May to September 1973. The variables include Ozone, Solar Radiation (Solar.R), Wind, and Temperature (Temp). We aim to predict Wind Speed based on these environmental factors. Wind speed is crucial for applications such as weather forecasting, aviation, and wind energy

Q2: I used three models: multiple linear regression (MLR), lasso, and random forest, to predict wind speed from temperature, solar radiation, and ozone levels. Multiple Linear Regression (MLR): This model showed temperature and ozone levels as significant predictors. The Mean Squared Prediction Error (MSPE) for this model was 8.98. Lasso Regression: Lasso showed similar results, confirming temperature and ozone levels as important predictors. The best lambda from cross-validation was 0.267, and the MSPE was 9.06. Random Forest: This non-linear model suggested that ozone levels were the most important predictor, followed by temperature, with solar radiation being the least important. The MSPE for random forest was 10.67.

Q3: The MLR and lasso models would suggest that the most important predictor of wind speed would be temperature. As the temperature lowered, wind speeds increased. The ozone levels variable was the second most important, and the MLR and lasso models suggest temperature as the most important predictor of wind speed, with ozone levels also contributing significantly. The lasso model set the coefficient of solar radiation to zero, which shows it has little to no correlation with wind speed. Our random forest models showed Ozone emerged as the most important predictor.

Conclusion: Temperature is the most influential predictor in linear models, while ozone levels are more influential in the random forest model, showing that the choice of model can change the interpretation of which predictor is most important.

Ethical Implications

The ethical implications of this work are mainly centered around the reliability of wind speed predictions. If these models were used in critical applications such as emergency response planning, inaccurate predictions could lead to poor decision-making. If these models were used in critical applications such as weather forecasting in New York, inaccurate predictions could lead to poor decision-making. For example, New York's public safety could be in danger if wind speed forecasts for storm conditions are incorrect, leading to delayed evacuations or poor preparation.

## RISHI INDIVIDUAL

1.  I want to find out if we can predict how much ozone is in the air on a given day just by looking at the weather. I’ll use information like how hot it is, how much sun there is, and how strong the wind is to make these predictions. I will use linear regression, random forest, and GAM to help me do this.

    ```{r}
    library(dplyr)
    library(ggplot2)

    airquality_clean <- na.omit(airquality)

    ```

### Linear Regression

```{r}
model_lm <- lm(Ozone ~ Solar.R + Wind + Temp, data = airquality_clean)
summary(model_lm)
plot(predict(model_lm), airquality_clean$Ozone,
     xlab = "Predicted Ozone", ylab = "Actual Ozone", main = "Linear Regression")
abline(0, 1, col = "red")
```

### Random Forest

```{r}
library(caret)
library(randomForest)

set.seed(123)
control <- trainControl(method = "cv", number = 10)

model_rf <- train(Ozone ~ Solar.R + Wind + Temp, 
                  data = airquality_clean, 
                  method = "rf", 
                  trControl = control)

print(model_rf)
varImp(model_rf)
```

### GAM

```{r}
library(mgcv)

model_gam <- gam(Ozone ~ s(Solar.R) + s(Wind) + s(Temp), data = airquality_clean)
summary(model_gam)
plot(model_gam, pages = 1, se = TRUE)
```

2.  Q1, Q2, and Q3

    Q1: Ozone pollution can have serious effects on human health and the environment. In this project, I explored whether we can predict daily ozone levels in New York City using weather data such as temperature, wind speed, and solar radiation. By identifying which weather conditions are most closely linked to higher ozone levels, we can better understand when air quality warnings might be needed.

    Q2: I used three different models to predict ozone levels based on the variables Temp, Wind, and Solar.r

    Linear Regression showed a positive relationship between temperature and ozone, but its predictions weren't very accurate. The predicted vs. actual plot showed a general trend but a wide spread of error, especially at higher ozone values.

    Random forest performed better. The variable importance plot showed that temperature was the most important predictor, followed by wind. Solar radiation had basically no influence in this model.

    GAM showed more detailed patterns. The plot of Temp showed a steep increase in ozone above 80°F, and the Wind plot showed that ozone drops sharply with increasing wind up to around 10 mph, then levels off. Solar.R had a very slight upward curve but remained relatively flat overall.

    Q3: All three models show that temperature and wind are the most useful weather variables for predicting ozone levels. Hot and calm days are likely to have higher ozone, while cooler or windier days have lower levels. These findings could help improve air quality warnings or guide public health recommendations during the summer.

    Ethical Implications: This project uses data dating back to 1973 however we could use the more recent years of weather data rather than all of it. It's also important to make sure that any predictions about air quality are used to protect the public health.

## WILL INDIVIDUAL

## Q1

I would like to see if principle component analysis (PCA) would be the best way to model wind speed, and if so, what would be the optimal amount of principle components for a model. PCA might be useful for this dataset because several of the predictors like Ozone, Solar.R, Temp, and Wind are likely to be correlated leading to multicollinearity in regression models. I will also fit ridge regression and a natural spline to see if there are better ways of modeling wind speed other than PCA.m

```{r}
    #Loading the dataset
    library(datasets)
    head(airquality)
    #Cleaning the dataset
    team_data_frame<-airquality
    team_data_frame$Month<- as.factor(team_data_frame$Month)
    team_data_frame$Day<- as.factor(team_data_frame$Day)
```

## Q2

### Ridge Regression

Ridge regression seemed to fit the data very well, and create a fairly accurate model. This model performed even better than my PCA model with 2 components, and gave me an MSE of about 6.94, using a lambda of just under 5. I think this is the best model that I created for this project.

```{r}
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(datasets))
data("airquality")
team_data_frame <- na.omit(airquality)
team_data_frame$Month <- as.factor(team_data_frame$Month)
team_data_frame$Day <- as.factor(team_data_frame$Day)
X <- model.matrix(Wind ~ ., data = team_data_frame)[, -1]
y <- team_data_frame$Wind
set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 5)
best_lambda <- cv_ridge$lambda.min
cat("Optimal lambda:", best_lambda, "\n")
predictions <- predict(cv_ridge, s = best_lambda, newx = X_test)
mse <- mean((predictions - y_test)^2)
cat("Test MSE:", mse, "\n")
library(ggplot2)
df <- data.frame(
  log_lambda = log(cv_ridge$lambda),
  mse = cv_ridge$cvm,
  se = cv_ridge$cvsd
)
ggplot(df, aes(x = log_lambda, y = mse)) +
  geom_line(color = "steelblue") +
  geom_point(color = "red") +
  geom_errorbar(aes(ymin = mse - se, ymax = mse + se), width = 0.1) +
  labs(
    title = "Ridge Regression: CV MSE vs log(Lambda)",
    x = "log(Lambda)",
    y = "Mean Squared Error (CV)"
  ) +
  theme_minimal()
```

### Principle Component Analysis (Supervised)

I chose to use principle component analysis to find the optimal number of principle components to use when predicting. Below I have found using 5 fold cross validation, that 2 principle components creates the most accurate model. This gave me an MSE of just over 8.

```{r}
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(pls))
suppressPackageStartupMessages(library(datasets))
data("airquality")
df <- na.omit(airquality)
df$Month <- as.factor(df$Month)
df$Day <- as.factor(df$Day)
set.seed(123)
train_index <- createDataPartition(df$Wind, p = 0.8, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
ctrl <- trainControl(method = "cv", number = 5)
set.seed(123)
suppressWarnings(pca_model <- train(
  Wind ~ ., 
  data = train_data,
  method = "pcr",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneLength = 10  # try up to 10 PCs
))
predictions <- predict(pca_model, newdata = test_data)
mse <- mean((predictions - test_data$Wind)^2)
cat("Test MSE:", mse, "\n")
plot(pca_model)
```

### Natural Spline

This model uses natural spline transformations on the continuous variables Ozone, Solar.R, and Temp to flexibly capture nonlinear relationships with Wind, while including categorical variables Month and Day as factors. Using 5-fold cross-validation, the best-performing model used 2 degrees of freedom for each spline and achieved a test MSE of approximately 10.97, indicating the average squared prediction error on unseen data.

```{r}
suppressPackageStartupMessages(library(splines))
data("airquality")
df <- na.omit(airquality)
df$Month <- as.factor(df$Month)
df$Day <- as.factor(df$Day)
set.seed(123)
train_index <- createDataPartition(df$Wind, p = 0.8, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
ctrl <- trainControl(method = "cv", number = 5)
df_grid <- expand.grid(df1 = 2:5, df2 = 2:5, df3 = 2:5)  # flexibility for each spline
best_mse <- Inf
best_model <- NULL
best_params <- NULL
for (i in 1:nrow(df_grid)) {
  d1 <- df_grid$df1[i]
  d2 <- df_grid$df2[i]
  d3 <- df_grid$df3[i]
  formula <- as.formula(
    paste("Wind ~ ns(Ozone,", d1, ") + ns(Solar.R,", d2, ") + ns(Temp,", d3, ") + Month + Day")
  )
  set.seed(123)
 suppressWarnings( model <- train(
    formula,
    data = train_data,
    method = "lm",
    trControl = ctrl
  ))
  pred <- predict(model, newdata = test_data)
  mse <- mean((pred - test_data$Wind)^2)
  if (mse < best_mse) {
    best_mse <- mse
    best_model <- model
    best_params <- c(d1 = d1, d2 = d2, d3 = d3)
  }
}
cat("Best degrees of freedom:\n")
print(best_params)
cat("Test MSE for best spline model:", best_mse, "\n")
```

## Q3

After completing the Q2 portion of this project, I believe that principle component analysis is not the best way to model wind speed in New York City. Ridge regression was the best model that I created with an MSE of about 6.93 and an optimal lambda of just under 5. While PCA was not the absolute best model I created, the model with 2 principle components was also not the worst that I created. The model with two principle components had an MSE of just over 8, meaning that while it didn't perform the best, it also didn't perform too poorly. The worst model I created was the natural spline model. Using 5 fold cross validation I determined that 2 degrees of freedom created the best model, however it still had an MSE Of 10.97 meaning that it performed worse than the other two models I created.

There could be some ethical implications with this work. While these models are fairly accurate, they are not perfect. Critical jobs rely on wind speed for everyday operations such as airports and construction companies. Under predicting wind could be dangerous in these situations, so we would need to make sure that our models are as accurate as possible.

#### What teams need to do

1.  Answer a team research question.

    Our team research question is "what model can be used to best predict wind in miles per hour in New York?" In order to do this, we fit 4 different models and compared their validation MSE to find which model had the best fit. Creating validation set:

    ```{r}
    set.seed(14)
    #Let's omit data with NAs
    team_data_frame <- na.omit(team_data_frame)


    #Using 15% of the data
    index<-sample(1:nrow(team_data_frame),ceiling(0.15*nrow(team_data_frame)))

    #creating training and validation data frames...
    training<-team_data_frame[-index,]
    validation<-team_data_frame[index,]
    ```

    Random Forest

    ```{r}
    library(caret)
    library(randomForest)

    airquality_clean <- na.omit(training)

    set.seed(123)
    control <- trainControl(method = "cv", number = 10)

    model_rf_wind <- train(Wind ~ Solar.R + Temp + Ozone + Month + Day,
                           data = airquality_clean,
                           method = "rf",
                           trControl = control)

    print(model_rf_wind)

    varImp(model_rf_wind)


    predict(model_rf_wind,newdata=validation)

    MSE <- mean((as.numeric(predictions) - as.numeric(validation$Wind))^2)

    cat("Random Forests Validation MSE is:", MSE, "\n")


    ```

    ## Will's Model Team Problem

I fit a BART to try to predict wind speed for the team section of our project. While the MSE was fairly low at a little over 9.5, I believe that other models are better at predicting wind speed. This dataset does not have many observations, or predictors which is generally needed for an accurate BART model. Despite this, it did seem to perform fairly well.

```{r}
suppressPackageStartupMessages(library(dbarts))
suppressPackageStartupMessages(library(caret))
data(airquality)
team_data_frame <- airquality
team_data_frame$Month <- as.factor(team_data_frame$Month)
team_data_frame$Day <- as.factor(team_data_frame$Day)
team_data_frame <- na.omit(team_data_frame)
X <- team_data_frame[, c("Ozone", "Solar.R", "Temp", "Month", "Day")] 
y <- team_data_frame$Wind 
set.seed(123)
folds <- createFolds(y, k = 5)
mse_list <- c()
for (i in 1:5) {
  test_idx <- folds[[i]]
  
  X_train <- X[-test_idx, ]
  y_train <- y[-test_idx]
  X_test <- X[test_idx, ]
  y_test <- y[test_idx]
  X_train_matrix <- model.matrix(~ . -1, data = X_train) 
  X_test_matrix <- model.matrix(~ . -1, data = X_test)
  bart_fit <- bart(
    x.train = X_train_matrix,
    y.train = y_train,
    x.test = X_test_matrix,
    keeptrees = FALSE
  )
  
  preds <- bart_fit$yhat.test.mean
  mse <- mean((preds - y_test)^2)
  
  mse_list <- c(mse_list, mse)
}

# Final average MSE
final_mse <- mean(mse_list)
cat("5-Fold CV MSE for BART model predicting Wind:", final_mse, "\n")

```

Fitting a Neural Network to model the data:

```{r}
# loading packages

library(ggplot2)
library(glmnet)
library(torch)
library(luz)
library(torchvision)
library(torchdatasets)
library(zeallot)
torch_manual_seed(13)

set.seed(13)

train_idx <- sample(1:nrow(team_data_frame), size = floor(0.8 * nrow(team_data_frame)))
train_df <- team_data_frame[train_idx, ]
test_df <- team_data_frame[-train_idx, ]

# One-hot encode Month and Day
train_onehot <- model.matrix(~ Month + Day - 1, data = train_df)
test_onehot <- model.matrix(~ Month + Day - 1, data = test_df)

# Normalize numeric variables
train_numeric <- scale(train_df[, c("Ozone", "Solar.R", "Temp")])
test_numeric <- scale(test_df[, c("Ozone", "Solar.R", "Temp")])

# Combine features
train_features <- cbind(train_numeric, train_onehot)
test_features <- cbind(test_numeric, test_onehot)

# Scale targets separately
train_target <- scale(train_df$Wind)
test_target <- scale(test_df$Wind)

# Convert to torch tensors
train_x <- torch_tensor(as.matrix(train_features), dtype = torch_float())
train_y <- torch_tensor(matrix(train_target, ncol = 1), dtype = torch_float())

test_x <- torch_tensor(as.matrix(test_features), dtype = torch_float())
test_y <- torch_tensor(matrix(test_target, ncol = 1), dtype = torch_float())




# Define the model
modelnn <- nn_module(
  initialize = function() {
    self$linear1 <- nn_linear(in_features = 38, out_features = 100)
    self$linear2 <- nn_linear(in_features = 100, out_features = 25)
    self$linear3 <- nn_linear(in_features = 25, out_features = 1)


    
    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%
      self$linear1() %>%
      self$activation() %>%
      
      self$linear2() %>%
      self$activation() %>%
      
      self$linear3()
  }  
)

# Setup the model
modelnn <- modelnn %>%
  setup(
    loss = nn_mse_loss(),  
    optimizer = optim_adam
  )


#Tuning the epochs

#initialize best rate
best_rate<-1000
for (i in 1:40){
# Fit the model
system.time(
  fitted <- modelnn %>%
    fit(
      data = list(x = train_x, y = train_y),
      epochs = i,
      valid_data = 0.2,
      dataloader_options = list(batch_size = 16),
      verbose = TRUE
    )
)
  
  predictions <- fitted %>% predict(test_x)
  as.numeric(predictions)
MSE <- mean((as.numeric(predictions) - as.numeric(test_y))^2)
if(MSE< best_rate){
  best_rate<-MSE
  best_epoch<-i
}}


#Finding validation MSE

# One-hot encode Month and Day
val_onehot <- model.matrix(~ Month + Day - 1, data = validation)

# Normalize numeric variables
val_numeric <- scale(validation[, c("Ozone", "Solar.R", "Temp")])


# Combine features
val_features <- cbind(val_numeric, val_onehot)

# Scale targets separately
val_target <- scale(validation$Wind)

# Convert to torch tensors
val_x <- torch_tensor(as.matrix(val_features), dtype = torch_float())
val_y <- torch_tensor(matrix(val_target, ncol = 1), dtype = torch_float())

system.time(
  fitted <- modelnn %>%
    fit(
      data = list(x = train_x, y = train_y),
      epochs = best_epoch,
      valid_data = 0.2,
      dataloader_options = list(batch_size = 16),
      verbose = TRUE
    )
)
  
  predictions <- fitted %>% predict(val_x)
  as.numeric(predictions)
MSE <- mean((as.numeric(predictions) - as.numeric(val_y))^2)

cat("Wind Speed (Scaled) Mean Squared Validation Error is: ", MSE, "\n")



#Unscaling the the MSE


# Unscale the predictions and true values
unscaled_predictions <- as.numeric(predictions) * attr(train_target, "scaled:scale") + attr(train_target, "scaled:center")
unscaled_test_y <- as.numeric(test_y) * attr(train_target, "scaled:scale") + attr(train_target, "scaled:center")

# Calculate MSE in original scale
MSE_original_scale <- mean((unscaled_predictions - unscaled_test_y)^2)

cat("Mean Squared Error unscaled: ", MSE_original_scale, "\n")
```

GAM

```{r}
library(mgcv)

# Clean 
team_data_frame <- na.omit(team_data_frame)
team_data_frame$Month <- factor(team_data_frame$Month)
team_data_frame$Day <- factor(team_data_frame$Day)



gam_model <- gam(Wind ~ s(Temp, bs = "cr") + s(Solar.R, bs = "cr") + s(Ozone, bs = "cr"), data = training)


summary(gam_model)
gam_predictions <- predict(gam_model, newdata = validation)

# Calculate MSE
gam_mse <- mean((validation$Wind - gam_predictions)^2)
cat("GAM Model MSE: ", gam_mse, "\n")
```

2.  Explain Q1, Q2, and Q3.

    The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data). Daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.

    -   `Ozone`: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

    -   `Solar.R`: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park

    -   `Wind`: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

    -   `Temp`: Maximum daily temperature in degrees Fahrenheit at LaGuardia Airport.

This data is part of the rdatasets package and is relatively out of date, but serves as an interesting data set to answer both our individual research questions and team research questions. We wanted to use this data set as a way to investigate different models learned throughout the semester. The four models we investigated for the team question were a neural network, BART, random forests, and a GAM. We used these models because they seemed to account for the non linearity of the data and could also use some underlying patterns that were not as clear to us initially. We created a validation test data set to measure MSE and use this as a metric of the best model in terms of prediction. After tuning the models, we found that the GAM had the lowest validation MSE and thus seemed to be the best model.

So, if we were to recommend a model to be used, we would recommend the GAM model presented in this project to be used for predictions. In addition, we would not recommend using a neural network as it had the worst MSE likely due to have a low amount of training observations. Other models could be investigated further, but for the purposes of this investigation, a GAM should be used to best predict the data.

3.  Comment on any ethical implications of this work.

    We analyzed the airquality dataset with somewhat outdated measurements. In addition, these measurements were made within a year and thus could not be very reflective of true patterns within these variables. This means that even though our GAM model predicts wind speed relatively well, we are not sure if this would translate to other data sets with the same variables. Therefore, if any of these models were sued to predict wind speeds to decide whether planes should take off at the LaGuardia airport, this could result in dangerous false predictions. If this happened, there could be catastrophic consequences in terms of plane crashes, delays to life saving treatments, etc. Other ethical implications include misuse of these models in other climate sciences. Climate scientists should not sue these models as sole predictors of wind speed as there may be may other underlying factors that impact wind speed. This could lead to woefully characterized statistics and papers in the future that could impact environmental safety in the years to come.

4.  Create at least one visualization that helps tell the overall narrative of the data.

    Below is a visualization of the MSEs of each model:

    ```{r}
    names<-c("Neural Network", "GAM", "Random Forests", "BART")
    mses<-c(18.34,4.48,9.54,8.30)
    mse_df<-data.frame("Model"=names,"MSE"=mses)
    ggplot(mse_df, aes(x = Model, y = MSE, fill = Model)) +
      geom_bar(stat = "identity") +
      labs(title = "Model Comparison by MSE",
           x = "Model",
           y = "Mean Squared Error") +
      theme_minimal() +
      theme(legend.position = "none")

    ```

See the above plots to see the model predictions with the validation data. Using MSE as our decision rule, we can see that a GAM would be the best model to predict this data. It's also helpful that a GAM is somewhat easy to interpret when compared to the other models so this is a plus.

#### Individual contributions

Each individual must comment on their contributions to the team research question. Only individuals who contribute to the team section (and state their contributions) will get points for the team section.

Josh: I created and trained the neural network model for the team research question. A neural network seemed like it could be useful because it may find underlying patterns in the daily data, though we did not have a lot of training observations so it's reliability was not going to be likely very high. I also helped in creating the MSE plot.

Rishi: I created the random forest model to help predict wind speed and helps answer the team research question. 

Chase: I created the gam model to help answer the team research question. The model ended seeming to be helpful when trying to predict wind speed in New York. 

Will: I created and trained the BART model to help predict wind speed in NYC and answer the team research question. The model performed fairly well despite lacking a large amount of variables and observations, which is normally helpful for BART models.
