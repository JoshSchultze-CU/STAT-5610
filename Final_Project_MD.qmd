---
title: "Final Project"
author: "Mountain Dewds"
format: pdf
editor: visual
date: 2025-04-18
---

## Final Project: Applying Various Methods to Answer Questions of Interest

**Due: 11:59PM Wednesday, April 30 on Canvas as a knitted pdf file of your team's Quarto document**

1.  You will individually fit **at least three statistical learning models** to answer your individual question of interest based on your team's chosen data set. You will individually explain what the relevant problem context is (Q1), what methods you used and their results (Q2), how those results answer the question of interest (Q3), and what implications (recommendations, next step actions, etc.) the answers have (Q3). In addition, you will comment on some ethical aspect of the problem.
2.  **Each individual must use at least one unique method**, i.e., not a method used by any of your teammates. For example, if team member 1 fit a cubic spline, BART, and a feedforward neural network to answer their individual research question and member 2 used a cubic spline, BART, and random forest, and member 3 used BART, natural spline, and lasso, then member 4 could not use a feedforward NN or a random forest or both natural spline and lasso.
3.  Teams will propose and answer a team research question that should incorporate the individual contributions. In other words, teams will combine findings from the individual components when answering their team question. Each individual must contribute to the team part and state what their contribution was. No stated contribution by individuals means no team points for that individual.
4.  Team answers should follow the Q1-Q2-Q3 workflow, including a reflection about an ethical principal used or some other ethical aspect of the team solution. Teams must also produce a plot that helps to tell the narrative of the data. (Individuals should also include plots in their individual parts, but it's not strictly required.)

#### What teams need to do first

1.  Choose a dataset that has enough interesting predictor X variables so that you can have four individual sub-research questions and one overall team research question. You are encouraged to gain practice doing unsupervised learning (PCA, K-means or hierarchical clustering) if that makes sense for your data set and your research questions.

    ```{r}
    #Loading the dataset
    library(datasets)
    head(airquality)


    #Cleaning the dataset

    team_data_frame<-airquality

    team_data_frame$Month<- as.factor(team_data_frame$Month)
    team_data_frame$Day<- as.factor(team_data_frame$Day)




    ```

2.  Brainstorm various research questions. Each individual needs their own sub-research question to answer. Think about how the answers to the sub-research questions could inform the answer to the overall team research question.

    Team research question: Our team research question is "what model can be used to best predict wind in miles per hour in New York?"

    Individual research questions:

    Josh: Can we reliably model wind speeds just using the temperature and what model would be best to do this? It seems that temperature could be a good predictor as temperature and wind speeds may be seasonal and somewhat correlated. For this question I will use generalized linear regression, a smoothing spline, polynomial regression, and local regression (polynomial ended up just acting as linear after tuning)

    Will:

    Chase:

    Rishi:

#### What individuals need to do

1.  Given your team's data set and your sub-research question, fit at least three statistical learning models. For each model fit, you should also play around with different tuning parameters, likely using CV.
2.  Explain Q1, Q2, and Q3.
3.  Comment on any ethical implications of this work.
4.  Highly recommended: Create visualizations of your models or just of the best model.

## JOSH INDIVIDUAL

1.  Given your team's data set and your sub-research question, fit at least three statistical learning models. For each model fit, you should also play around with different tuning parameters, likely using CV.

    My sub-research question is: can we reliably model wind speeds just using the temperature and what model would be best to do this? It seems that temperature could be a good predictor as temperature and wind speeds may be seasonal and somewhat correlated. For this question I will use generalized linear regression, a smoothing spline, polynomial regression, and local regression (polynomial ended up just acting as linear after tuning)

    First I created my own data frame to investigate this research question:

    ```{r}
    set.seed(44)
    library(ggplot2)
    josh_research_df<-data.frame("Wind"=team_data_frame$Wind,"Temp"=team_data_frame$Temp)

    #Creating training and validation set
    #Used 15% of data for validation

    index<-sample(1:nrow(josh_research_df),ceiling(0.15*nrow(josh_research_df)))

    js_validation_data<-josh_research_df[index,]
    js_training_data<-josh_research_df[-index,]
    ```

    Generalized Linear Regression

    ```{r}
    set.seed(44)
    lmod<-lm(Wind~Temp, data=js_training_data)

    wind_pred<-predict(lmod,newdata=js_validation_data)

    js_validation_data$LMOD_Predictions<-wind_pred

    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=LMOD_Predictions),color="green")

    MSPE<-mean((js_validation_data$Wind-wind_pred)^2)
    MSPE
    ```

    Smoothing Spline

    ```{r}

    library(splines)
    library(dplyr)

    set.seed(44)
    ##K-fold CV
    k=10
    n=nrow(js_training_data)
    width=n/k
    test_indexes<-sample(1:n,replace=FALSE,n)
    natural_MSE<-matrix(NA,ncol=6,nrow=10)
    for (i in 1:k){
    test<-data.frame("Wind"=js_training_data[(test_indexes[as.integer(width*(i-1) ):as.integer(width*i)]),1],"Temp"=js_training_data[test_indexes[as.integer(width*(i-1) ):as.integer(width*i)],2])
    test_wind<-data.frame("Temp"=test$Temp)
    train_index<-setdiff(test_indexes,test_indexes[(width*(i-1) ):(width*i)])
    train<-data.frame("Wind"=js_training_data[train_index,1],"Temp"=js_training_data[train_index,2])



    for (j in 1:6){
    degrees_of_freedom=j
    natural_spline<-lm(data=js_training_data,Wind~ns(Temp, df=degrees_of_freedom))
    natural_pred<-predict(natural_spline,newdata=test_wind)
    natural_MSE[i,j]<-mean((test$Wind-natural_pred)^2)
    }
    }

    which(colMeans(natural_MSE)==min(colMeans(natural_MSE)))

    degrees_of_freedom=6
    natural_spline<-lm(data=js_training_data,Wind~ns(Temp, df=degrees_of_freedom))
    natural_pred<-predict(natural_spline,newdata=js_validation_data)
    MSPE<-mean((js_validation_data$Wind-natural_pred)^2)
    MSPE

    js_validation_data$natural_predictions<-natural_pred
    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=natural_predictions),color="yellow")


    ```

    Polynomial Regression: For the polynomial regression, I used k=10 as it divided into the number of rows easily. Note that this ended up just being linear regression, so I fit one more model.

    ```{r}
    set.seed(44)
    k=10
    n=nrow(js_training_data)
    width=n/k
    test_indexes<-sample(1:n,replace=FALSE,n)
    poly_mse<-matrix(NA,ncol=5,nrow=10)
    for (i in 1:k){
    test<-data.frame("Wind"=js_training_data[(test_indexes[as.integer(width*(i-1) ):as.integer(width*i)]),1],"Temp"=js_training_data[test_indexes[as.integer(width*(i-1) ):as.integer(width*i)],2])
    test_wind<-data.frame("Temp"=test$Temp)
    train_index<-setdiff(test_indexes,test_indexes[(width*(i-1) ):(width*i)])
    train<-data.frame("Wind"=js_training_data[train_index,1],"Temp"=js_training_data[train_index,2])

    #initializing MSE over each fold

    for (j in 1:5){
    poly_degree=j
    polyfit<- lm(data=train, Wind~ poly(Temp,poly_degree))
    poly_pred<-predict(polyfit,newdata=test_wind)
    poly_mse[i,j]<-mean((test$Wind-poly_pred)^2)
    }

    }

    #finding best average MSE for polynomial degree

    which(colMeans(poly_mse)==min(colMeans(poly_mse)))

    #This still results in a polynomial of the 1st degree to be the best. So we should use basically a linear regression.


    poly_degree=1
    polyfit<- lm(data=js_training_data, Wind~ poly(Temp,poly_degree))
    poly_pred<-predict(polyfit,newdata=js_validation_data)
    #Mean squared prediction error with validation Data
    MSPE<-mean((js_validation_data$Wind-poly_pred)^2)
    MSPE

    js_validation_data$poly_predictions<-poly_pred

    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=poly_predictions),color="green")

    ```

    Local Regression:

    ```{r}
    set.seed(44)
    k=10
    n=nrow(js_training_data)
    width=n/k
    test_indexes<-sample(1:n,replace=FALSE,n)
    local_mse<-matrix(NA,ncol=5,nrow=10)
    for (i in 1:k){
    test<-data.frame("Wind"=js_training_data[(test_indexes[(width*(i-1) ):(width*i)]),1],"Temp"=js_training_data[test_indexes[(width*(i-1) ):(width*i)],2])
    test_wind<-data.frame("Temp"=test$Temp)
    train_index<-setdiff(test_indexes,test_indexes[(width*(i-1) ):(width*i)])
    train<-data.frame("Wind"=js_training_data[train_index,1],"Temp"=js_training_data[train_index,2])

    j_tune<-seq(0.1,0.5, by=0.1)
    for (j in j_tune){
    span_val=j
    local_regression_fit<-loess(Wind~Temp,data=train, span=span_val)
    local_pred<-predict(local_regression_fit,newdata=test_wind)
    local_mse[i,j*10]<-mean((test$Wind-local_pred)^2)
    }
    }

    which(colMeans(local_mse[c(2,3,5,6,7,8,9,10),])==min(colMeans(local_mse[c(2,3,5,6,7,8,9,10),])))


    #Indicating errors with
    span_val=0.5
    local_regression_fit<-loess(Wind~Temp,data=js_training_data, span=span_val)
    local_pred<-predict(local_regression_fit,newdata=js_validation_data)

    #Mean squared prediction error with validation Data
    MSPE<-mean((js_validation_data$Wind-local_pred)^2)
    MSPE

    js_validation_data$loess_predictions<-local_pred

    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=loess_predictions),color="red")

    ```

    So out of these 4 methods of fitting wind to temperature, we were not able to reliably find a high accuracy model and surprisingly, the linear model performed the best!

2.  Explain Q1, Q2, and Q3.

3.  Comment on any ethical implications of this work.

4.  Highly recommended: Create visualizations of your models or just of the best model.

    Below is a final visualization of all the models together:

    ```{r}
    ggplot(data=js_validation_data)+geom_point(aes(x=Temp,y=Wind))+geom_line(aes(x=Temp,y=natural_predictions),color="blue")+geom_line(aes(x=Temp,y=loess_predictions),color="red")+geom_line(aes(x=Temp,y=poly_predictions),color="green")+geom_line(aes(x=Temp,y=natural_predictions),color="yellow")
    ```

#### What teams need to do

1.  Answer a team research question.

    Our team research question is "what model can be used to best predict wind in miles per hour in New York?" In order to do this, we fit 4 different models and compared their validation MSE to find which model had the best fit.

    Creating a validation set:

    ```{r}

    #Let's omit data with NAs
    team_data_frame <- na.omit(team_data_frame)


    #Using 15% of the data
    index<-sample(1:nrow(team_data_frame),ceiling(0.15*nrow(team_data_frame)))

    #creating training and validation data frames...
    training<-team_data_frame[-index,]
    validation<-team_data_frame[index,]
    ```

    Fitting a Neural Network to model the data:

    ```{r}
    # loading packages

    library(ggplot2)
    library(glmnet)
    library(torch)
    library(luz)
    library(torchvision)
    library(torchdatasets)
    library(zeallot)
    torch_manual_seed(13)

    set.seed(13)

    train_idx <- sample(1:nrow(team_data_frame), size = floor(0.8 * nrow(team_data_frame)))
    train_df <- team_data_frame[train_idx, ]
    test_df <- team_data_frame[-train_idx, ]

    # One-hot encode Month and Day
    train_onehot <- model.matrix(~ Month + Day - 1, data = train_df)
    test_onehot <- model.matrix(~ Month + Day - 1, data = test_df)

    # Normalize numeric variables
    train_numeric <- scale(train_df[, c("Ozone", "Solar.R", "Temp")])
    test_numeric <- scale(test_df[, c("Ozone", "Solar.R", "Temp")])

    # Combine features
    train_features <- cbind(train_numeric, train_onehot)
    test_features <- cbind(test_numeric, test_onehot)

    # Scale targets separately
    train_target <- scale(train_df$Wind)
    test_target <- scale(test_df$Wind)

    # Convert to torch tensors
    train_x <- torch_tensor(as.matrix(train_features), dtype = torch_float())
    train_y <- torch_tensor(matrix(train_target, ncol = 1), dtype = torch_float())

    test_x <- torch_tensor(as.matrix(test_features), dtype = torch_float())
    test_y <- torch_tensor(matrix(test_target, ncol = 1), dtype = torch_float())




    # Define the model
    modelnn <- nn_module(
      initialize = function() {
        self$linear1 <- nn_linear(in_features = 38, out_features = 100)
        self$linear2 <- nn_linear(in_features = 100, out_features = 25)
        self$linear3 <- nn_linear(in_features = 25, out_features = 1)


        
        self$activation <- nn_relu()
      },
      forward = function(x) {
        x %>%
          self$linear1() %>%
          self$activation() %>%
          
          self$linear2() %>%
          self$activation() %>%
          
          self$linear3()
      }  
    )

    # Setup the model
    modelnn <- modelnn %>%
      setup(
        loss = nn_mse_loss(),  
        optimizer = optim_adam
      )


    #Tuning the epochs

    #initialize best rate
    best_rate<-1000
    for (i in 1:40){
    # Fit the model
    system.time(
      fitted <- modelnn %>%
        fit(
          data = list(x = train_x, y = train_y),
          epochs = i,
          valid_data = 0.2,
          dataloader_options = list(batch_size = 16),
          verbose = TRUE
        )
    )
      
      predictions <- fitted %>% predict(test_x)
      as.numeric(predictions)
    MSE <- mean((as.numeric(predictions) - as.numeric(test_y))^2)
    if(MSE< best_rate){
      best_rate<-MSE
      best_epoch<-i
    }}


    #Finding validation MSE

    # One-hot encode Month and Day
    val_onehot <- model.matrix(~ Month + Day - 1, data = validation)

    # Normalize numeric variables
    val_numeric <- scale(validation[, c("Ozone", "Solar.R", "Temp")])


    # Combine features
    val_features <- cbind(val_numeric, val_onehot)

    # Scale targets separately
    val_target <- scale(validation$Wind)

    # Convert to torch tensors
    val_x <- torch_tensor(as.matrix(val_features), dtype = torch_float())
    val_y <- torch_tensor(matrix(val_target, ncol = 1), dtype = torch_float())

    system.time(
      fitted <- modelnn %>%
        fit(
          data = list(x = train_x, y = train_y),
          epochs = best_epoch,
          valid_data = 0.2,
          dataloader_options = list(batch_size = 16),
          verbose = TRUE
        )
    )
      
      predictions <- fitted %>% predict(val_x)
      as.numeric(predictions)
    MSE <- mean((as.numeric(predictions) - as.numeric(val_y))^2)

    cat("Wind Speed (Scaled) Mean Squared Validation Error is: ", MSE, "\n")



    #Unscaling the the MSE


    # Unscale the predictions and true values
    unscaled_predictions <- as.numeric(predictions) * attr(train_target, "scaled:scale") + attr(train_target, "scaled:center")
    unscaled_test_y <- as.numeric(test_y) * attr(train_target, "scaled:scale") + attr(train_target, "scaled:center")

    # Calculate MSE in original scale
    MSE_original_scale <- mean((unscaled_predictions - unscaled_test_y)^2)

    cat("Mean Squared Error unscaled: ", MSE_original_scale, "\n")

    ```

2.  Explain Q1, Q2, and Q3.

    The data were obtained from the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data). Daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.

    -   `Ozone`: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

    -   `Solar.R`: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park

    -   `Wind`: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

    -   `Temp`: Maximum daily temperature in degrees Fahrenheit at LaGuardia Airport.

This data is part of the rdatasets package and is relatively out of date, but serves as an interesting data set to answer both our individual research questions and team research questions.

3.  Comment on any ethical implications of this work.
4.  Create at least one visualization that helps tell the overall narrative of the data.

#### Individual contributions

Each individual must comment on their contributions to the team research question. Only individuals who contribute to the team section (and state their contributions) will get points for the team section.

#### **Some intended outcomes from this final project:**

-   You will individually get practice fitting statistical learning models
-   You will consolidate your learning of statistical learning methods to answer research questions of interest to you
-   You will gain experience working on a data science project as a team
-   You can post your final project on a personal portfolio (like a GitHub page) to show an example of your work to potential employers
